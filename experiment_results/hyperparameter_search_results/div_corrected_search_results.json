{
  "problem_setup": {
    "interpolation_range": "[-2.0,2.0]",
    "extrapolation_range": "[[-6.0,-2.0],[2.0,6.0]]",
    "note": "This is the CORRECTED hard setup matching user args"
  },
  "search_space": {
    "learning_rate": [
      1e-06,
      5e-06,
      1e-05,
      2e-05,
      5e-05
    ],
    "clip_grad_norm": [
      0.001,
      0.005,
      0.01,
      0.05
    ],
    "batch_size": [
      64,
      128,
      256
    ],
    "max_target_magnitude": [
      5.0,
      10.0,
      20.0,
      50.0
    ],
    "seed": [
      42,
      123,
      456,
      789,
      1123,
      2024,
      3141,
      5678
    ],
    "num_subsets": [
      1,
      2
    ],
    "div_regularizer": [
      0.001,
      0.01,
      0.1
    ]
  },
  "ultra_conservative_configs": [
    {
      "learning_rate": 1e-06,
      "clip_grad_norm": 0.001,
      "batch_size": 64,
      "max_target_magnitude": 5.0,
      "seed": 42,
      "num_subsets": 1,
      "div_regularizer": 0.1
    },
    {
      "learning_rate": 5e-06,
      "clip_grad_norm": 0.005,
      "batch_size": 128,
      "max_target_magnitude": 10.0,
      "seed": 123,
      "num_subsets": 1,
      "div_regularizer": 0.01
    },
    {
      "learning_rate": 1e-05,
      "clip_grad_norm": 0.01,
      "batch_size": 128,
      "max_target_magnitude": 10.0,
      "seed": 456,
      "num_subsets": 2,
      "div_regularizer": 0.001
    },
    {
      "learning_rate": 1e-05,
      "clip_grad_norm": 0.01,
      "batch_size": 128,
      "max_target_magnitude": 10.0,
      "seed": 789,
      "num_subsets": 2,
      "div_regularizer": 0.001
    },
    {
      "learning_rate": 1e-05,
      "clip_grad_norm": 0.01,
      "batch_size": 128,
      "max_target_magnitude": 10.0,
      "seed": 1123,
      "num_subsets": 2,
      "div_regularizer": 0.001
    },
    {
      "learning_rate": 2e-05,
      "clip_grad_norm": 0.01,
      "batch_size": 256,
      "max_target_magnitude": 20.0,
      "seed": 2024,
      "num_subsets": 1,
      "div_regularizer": 0.01
    }
  ],
  "results": [
    {
      "experiment_id": 0,
      "config": {
        "learning_rate": 1e-06,
        "clip_grad_norm": 0.001,
        "batch_size": 64,
        "max_target_magnitude": 5.0,
        "seed": 42,
        "num_subsets": 1,
        "div_regularizer": 0.1
      },
      "success": false,
      "grokked": false,
      "early_stopped": false,
      "duration": 10.370837688446045,
      "final_losses": {
        "train": Infinity,
        "interpolation": Infinity,
        "extrapolation": Infinity
      },
      "stdout": "running\n  - layer_type: -1\n  - layer_type: DAG\n  - first_layer: None\n  - operation: div\n  - num_subsets: 1\n  - regualizer: 10\n  - regualizer_z: 0\n  - regualizer_oob: 1\n  -\n  - max_iterations: 3000\n  - batch_size: 64\n  - seed: 42\n  -\n  - interpolation_range: [-2.0, 2.0]\n  - extrapolation_range: [[-6.0, -2.0], [2.0, 6.0]]\n  - input_size: 2\n  - output_size: 1\n  - subset_ratio: 0.5\n  - overlap_ratio: 0.0\n  - simple: False\n  -\n  - hidden_size: 2\n  - nac_mul: none\n  - oob_mode: clip\n  - regualizer_scaling: linear\n  - regualizer_scaling_start: 1000000\n  - regualizer_scaling_end: 2000000\n  - regualizer_shape: linear\n  - mnac_epsilon: 0\n  - nalu_bias: False\n  - nalu_two_nac: False\n  - nalu_two_gate: False\n  - nalu_mul: normal\n  - nalu_gate: normal\n  - nac_weight: normal\n  -\n  - optimizer: adam\n  - learning_rate: 1e-06\n  - momentum: 0.0\n  -\n  - cuda: False\n  - name_prefix: simple_function_static\n  - remove_existing_data: False\n  - verbose: False\n  -\n  - reg_scale_type: heim\n  - regualizer_beta_start: 1e-05\n  - regualizer_beta_end: 0.0001\n  - regualizer_beta_step: 10000\n  - regualizer_beta_growth: 10\n  - regualizer_l1: False\n  - regualizer-npu-w: 0\n  - regualizer-gate: 0\n  - npu-clip: none\n  - npu-Wr-init: xavier-uniform\n  -\n  - pytorch-precision: torch.float32\n  -\n  - no-save: False\n  - load-checkpoint: False\n  - log-interval: 500\n  -\n  - clip-grad-norm: 0.001\n  - nru_div_mode: div\n  - realnpu_reg_type: W\n  -\n  - reinit: False\n  - reinit_epoch_interval: 10\n  - reinit_max_stored_losses: 5000\n  - reinit_loss_thr: 1.0\n  - num_bins: 5\n  -\n  -\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - corrected_search_0\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/40eagljt\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_183336-40eagljt/logs\u001b[0m\n",
      "stderr": "agljt\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\n"
    },
    {
      "experiment_id": 1,
      "config": {
        "learning_rate": 5e-06,
        "clip_grad_norm": 0.005,
        "batch_size": 128,
        "max_target_magnitude": 10.0,
        "seed": 123,
        "num_subsets": 1,
        "div_regularizer": 0.01
      },
      "success": false,
      "grokked": false,
      "early_stopped": false,
      "duration": 10.37238883972168,
      "final_losses": {
        "train": Infinity,
        "interpolation": Infinity,
        "extrapolation": Infinity
      },
      "stdout": "running\n  - layer_type: -1\n  - layer_type: DAG\n  - first_layer: None\n  - operation: div\n  - num_subsets: 1\n  - regualizer: 10\n  - regualizer_z: 0\n  - regualizer_oob: 1\n  -\n  - max_iterations: 3000\n  - batch_size: 128\n  - seed: 123\n  -\n  - interpolation_range: [-2.0, 2.0]\n  - extrapolation_range: [[-6.0, -2.0], [2.0, 6.0]]\n  - input_size: 2\n  - output_size: 1\n  - subset_ratio: 0.5\n  - overlap_ratio: 0.0\n  - simple: False\n  -\n  - hidden_size: 2\n  - nac_mul: none\n  - oob_mode: clip\n  - regualizer_scaling: linear\n  - regualizer_scaling_start: 1000000\n  - regualizer_scaling_end: 2000000\n  - regualizer_shape: linear\n  - mnac_epsilon: 0\n  - nalu_bias: False\n  - nalu_two_nac: False\n  - nalu_two_gate: False\n  - nalu_mul: normal\n  - nalu_gate: normal\n  - nac_weight: normal\n  -\n  - optimizer: adam\n  - learning_rate: 5e-06\n  - momentum: 0.0\n  -\n  - cuda: False\n  - name_prefix: simple_function_static\n  - remove_existing_data: False\n  - verbose: False\n  -\n  - reg_scale_type: heim\n  - regualizer_beta_start: 1e-05\n  - regualizer_beta_end: 0.0001\n  - regualizer_beta_step: 10000\n  - regualizer_beta_growth: 10\n  - regualizer_l1: False\n  - regualizer-npu-w: 0\n  - regualizer-gate: 0\n  - npu-clip: none\n  - npu-Wr-init: xavier-uniform\n  -\n  - pytorch-precision: torch.float32\n  -\n  - no-save: False\n  - load-checkpoint: False\n  - log-interval: 500\n  -\n  - clip-grad-norm: 0.005\n  - nru_div_mode: div\n  - realnpu_reg_type: W\n  -\n  - reinit: False\n  - reinit_epoch_interval: 10\n  - reinit_max_stored_losses: 5000\n  - reinit_loss_thr: 1.0\n  - num_bins: 5\n  -\n  -\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - corrected_search_1\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/sp4cyyea\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_183336-sp4cyyea/logs\u001b[0m\n",
      "stderr": "cyyea\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\n"
    },
    {
      "experiment_id": 2,
      "config": {
        "learning_rate": 1e-05,
        "clip_grad_norm": 0.01,
        "batch_size": 128,
        "max_target_magnitude": 10.0,
        "seed": 456,
        "num_subsets": 2,
        "div_regularizer": 0.001
      },
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "duration": 155.69099974632263,
      "final_losses": {
        "train": 3.7529096603,
        "interpolation": 5.9935436249,
        "extrapolation": 2.4108169079
      },
      "stdout": ".0, 0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, -0.0, 0.0, 0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.0, 0.0, -0.0, -0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 1.0, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.09095, 0.01746, 0.01008]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [1.0, 1.0, 0.0]\n\tselected_value (hardened eval): 1.0\nSample statistics (SOFT training state):\ninput=[0.31713, 0.71821]\noutput=0.15744, target=0.44155\nG (soft training): [0.51111, 0.51911, 0.49635]\n    Step 0:\n        selectors: [0.00529, -0.02542, -0.0, -0.0, -0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.00151, 0.02603, 0.00137, 0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 1.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [0.00729, -0.01656, 0.01081, 0.00628, 0.0]\n        inputs:    [3.73165, -3.78359, 1.0, 1.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [-0.06757, -0.04406, 0.0174]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.01086, 0.48539, 0.00364]\n\tselected_value (soft training): 0.00364\nfinished:\n  - loss_train_capped: 4.489047527313232\n  - loss_train (+reg loss): 3.7529096603393555\n  - loss_train_criterion: 3.7529096603393555\n  - loss_valid_inter: 5.99354362487793\n  - loss_valid_extra: 2.4108169078826904\n\nModel (/checkpoint)  trained for 3000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - corrected_search_2\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ur67oxc1\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_183336-ur67oxc1/logs\u001b[0m\n",
      "stderr": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_183336-ur67oxc1\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - corrected_search_2\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ur67oxc1\n"
    },
    {
      "experiment_id": 4,
      "config": {
        "learning_rate": 1e-05,
        "clip_grad_norm": 0.01,
        "batch_size": 128,
        "max_target_magnitude": 10.0,
        "seed": 1123,
        "num_subsets": 2,
        "div_regularizer": 0.001
      },
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "duration": 151.46892094612122,
      "final_losses": {
        "train": 5.0770263672,
        "interpolation": 4.9370102882,
        "extrapolation": 1.4504537582
      },
      "stdout": ", 0.0, 0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [-0.0, 0.0, -0.0, -0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.01025, 0.05573, -0.0253]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, 0.0, 1.0]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[1.57378, -0.73128]\noutput=0.16955, target=-2.15209\nG (soft training): [0.50287, 0.50151, 0.49719]\n    Step 0:\n        selectors: [0.01309, 0.0017, 0.0, -0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.00144, 0.00513, -0.00694, -0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [-0.02681, 0.00618, -0.02402, -0.00435, -0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.01025, 0.05573, -0.0253]\n\tselected_node: 1\n\tintermediate_values (soft training): [0.50959, -0.00154, 0.00274]\n\tselected_value (soft training): -0.00154\nfinished:\n  - loss_train_capped: 5.027690887451172\n  - loss_train (+reg loss): 5.0770263671875\n  - loss_train_criterion: 5.0770263671875\n  - loss_valid_inter: 4.937009334564209\n  - loss_valid_extra: 1.450453758239746\n\nModel (/checkpoint)  trained for 3000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - corrected_search_4\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/0e83h251\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_183345-0e83h251/logs\u001b[0m\n",
      "stderr": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_183345-0e83h251\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - corrected_search_4\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/0e83h251\n"
    },
    {
      "experiment_id": 3,
      "config": {
        "learning_rate": 1e-05,
        "clip_grad_norm": 0.01,
        "batch_size": 128,
        "max_target_magnitude": 10.0,
        "seed": 789,
        "num_subsets": 2,
        "div_regularizer": 0.001
      },
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "duration": 152.46554398536682,
      "final_losses": {
        "train": 5.8953385353,
        "interpolation": 5.4737286568,
        "extrapolation": 1.9221665859
      },
      "stdout": "0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [0.0, -0.0, -0.0, -0.0, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.0392, 0.03083, -0.01874]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.0, 0.0, 0.0]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[1.38391, -1.43414]\noutput=0.16814, target=-0.96497\nG (soft training): [0.50291, 0.50243, 0.50333]\n    Step 0:\n        selectors: [0.00147, 0.00402, -0.0, 0.0, -0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.00594, 0.02668, -0.01731, 0.0, -0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [0.01691, -0.01775, -0.00938, -0.02081, 0.0]\n        inputs:    [3.73165, -3.78359, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.0392, 0.03083, -0.01874]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.00291, -0.00394, 0.53022]\n\tselected_value (soft training): -0.00291\nfinished:\n  - loss_train_capped: 6.177368640899658\n  - loss_train (+reg loss): 5.895338535308838\n  - loss_train_criterion: 5.895338535308838\n  - loss_valid_inter: 5.473729133605957\n  - loss_valid_extra: 1.9221665859222412\n\nModel (/checkpoint)  trained for 3000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - corrected_search_3\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/jcaa46wg\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_183345-jcaa46wg/logs\u001b[0m\n",
      "stderr": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_183345-jcaa46wg\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - corrected_search_3\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/jcaa46wg\n"
    },
    {
      "experiment_id": 5,
      "config": {
        "learning_rate": 2e-05,
        "clip_grad_norm": 0.01,
        "batch_size": 256,
        "max_target_magnitude": 20.0,
        "seed": 2024,
        "num_subsets": 1,
        "div_regularizer": 0.01
      },
      "success": false,
      "grokked": false,
      "early_stopped": false,
      "duration": 7.891513109207153,
      "final_losses": {
        "train": Infinity,
        "interpolation": Infinity,
        "extrapolation": Infinity
      },
      "stdout": "running\n  - layer_type: -1\n  - layer_type: DAG\n  - first_layer: None\n  - operation: div\n  - num_subsets: 1\n  - regualizer: 10\n  - regualizer_z: 0\n  - regualizer_oob: 1\n  -\n  - max_iterations: 3000\n  - batch_size: 256\n  - seed: 2024\n  -\n  - interpolation_range: [-2.0, 2.0]\n  - extrapolation_range: [[-6.0, -2.0], [2.0, 6.0]]\n  - input_size: 2\n  - output_size: 1\n  - subset_ratio: 0.5\n  - overlap_ratio: 0.0\n  - simple: False\n  -\n  - hidden_size: 2\n  - nac_mul: none\n  - oob_mode: clip\n  - regualizer_scaling: linear\n  - regualizer_scaling_start: 1000000\n  - regualizer_scaling_end: 2000000\n  - regualizer_shape: linear\n  - mnac_epsilon: 0\n  - nalu_bias: False\n  - nalu_two_nac: False\n  - nalu_two_gate: False\n  - nalu_mul: normal\n  - nalu_gate: normal\n  - nac_weight: normal\n  -\n  - optimizer: adam\n  - learning_rate: 2e-05\n  - momentum: 0.0\n  -\n  - cuda: False\n  - name_prefix: simple_function_static\n  - remove_existing_data: False\n  - verbose: False\n  -\n  - reg_scale_type: heim\n  - regualizer_beta_start: 1e-05\n  - regualizer_beta_end: 0.0001\n  - regualizer_beta_step: 10000\n  - regualizer_beta_growth: 10\n  - regualizer_l1: False\n  - regualizer-npu-w: 0\n  - regualizer-gate: 0\n  - npu-clip: none\n  - npu-Wr-init: xavier-uniform\n  -\n  - pytorch-precision: torch.float32\n  -\n  - no-save: False\n  - load-checkpoint: False\n  - log-interval: 500\n  -\n  - clip-grad-norm: 0.01\n  - nru_div_mode: div\n  - realnpu_reg_type: W\n  -\n  - reinit: False\n  - reinit_epoch_interval: 10\n  - reinit_max_stored_losses: 5000\n  - reinit_loss_thr: 1.0\n  - num_bins: 5\n  -\n  -\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - corrected_search_5\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/8gq1x3vb\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_183610-8gq1x3vb/logs\u001b[0m\n",
      "stderr": "1x3vb\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\n"
    }
  ],
  "grokked_configs": [],
  "good_progress_configs": [],
  "summary": {
    "total_experiments": 6,
    "successful_experiments": 3,
    "grokking_experiments": 0,
    "good_progress_experiments": 0,
    "grokking_rate": 0.0,
    "good_progress_rate": 0.0
  }
}