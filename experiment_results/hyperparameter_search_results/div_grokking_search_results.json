{
  "search_space": {
    "learning_rate": [
      1e-05,
      5e-05,
      0.0001,
      0.0002,
      0.0005,
      0.001
    ],
    "clip_grad_norm": [
      0.001,
      0.01,
      0.1,
      1.0,
      10.0
    ],
    "regualizer": [
      1.0,
      5.0,
      10.0,
      50.0,
      100.0
    ],
    "regualizer_z": [
      0.0,
      1.0,
      10.0
    ],
    "regualizer_oob": [
      0.1,
      1.0,
      10.0
    ],
    "hidden_size": [
      1,
      2,
      4
    ],
    "batch_size": [
      32,
      64,
      128,
      256
    ],
    "optimizer": [
      "adam",
      "sgd"
    ],
    "momentum": [
      0.0,
      0.9
    ],
    "num_subsets": [
      1,
      2,
      3
    ],
    "max_target_magnitude": [
      10.0,
      100.0,
      1000.0
    ],
    "div_regularizer": [
      null,
      1e-06,
      0.0001,
      0.01
    ]
  },
  "results": [
    {
      "experiment_id": 0,
      "config": {
        "learning_rate": 0.0001,
        "clip_grad_norm": 0.01,
        "regualizer": 50.0,
        "regualizer_z": 10.0,
        "regualizer_oob": 10.0,
        "hidden_size": 2,
        "batch_size": 128,
        "optimizer": "adam",
        "momentum": 0.0,
        "num_subsets": 2,
        "max_target_magnitude": 100.0,
        "div_regularizer": 0.0001
      },
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "duration": 98.49264216423035,
      "final_losses": {
        "train": 0.0335825942,
        "interpolation": 0.8361894488,
        "extrapolation": 0.86066854
      },
      "stdout": "-0.0, 0.0, 0.0, 0.0]\n        inputs:    [2.16913, 3.36666, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [2.16913, 3.36666, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 0.0, 0.0]\n        inputs:    [2.16913, 3.36666, 0.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.36736, 0.33155, -0.3631]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, 1.0, 1.0]\n\tselected_value (hardened eval): 1.0\nSample statistics (SOFT training state):\ninput=[1.49196, 1.40261]\noutput=1.12696, target=1.0637\nG (soft training): [0.40901, 0.46094, 0.50908]\n    Step 0:\n        selectors: [0.52915, 0.053, 0.0, 0.0, -0.0]\n        inputs:    [2.16913, 3.36666, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.13489, -0.08993, 0.09893, 0.0, 0.0]\n        inputs:    [2.16913, 3.36666, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [0.42868, 0.10217, 0.45709, -0.28779, -0.0]\n        inputs:    [2.16913, 3.36666, 0.0, 1.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.26677, -0.32775, 0.49395]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.16375, 0.04165, 1.57485]\n\tselected_value (soft training): 1.57485\nfinished:\n  - loss_train_capped: 0.9138168096542358\n  - loss_train (+reg loss): 0.033582594245672226\n  - loss_train_criterion: 0.033582594245672226\n  - loss_valid_inter: 0.8361894488334656\n  - loss_valid_extra: 0.8606685400009155\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - hypersearch_0\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/75f1ocjl\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_174249-75f1ocjl/logs\u001b[0m\n",
      "stderr": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_174249-75f1ocjl\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - hypersearch_0\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/75f1ocjl\nwandb: WARNING Tried to log to step 2000 that is less than the current step 2001. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
    },
    {
      "experiment_id": 1,
      "config": {
        "learning_rate": 5e-05,
        "clip_grad_norm": 0.1,
        "regualizer": 100.0,
        "regualizer_z": 1.0,
        "regualizer_oob": 1.0,
        "hidden_size": 1,
        "batch_size": 64,
        "optimizer": "adam",
        "momentum": 0.0,
        "num_subsets": 1,
        "max_target_magnitude": 10.0,
        "div_regularizer": 1e-06
      },
      "success": false,
      "grokked": false,
      "early_stopped": false,
      "duration": 8.262529134750366,
      "final_losses": {
        "train": Infinity,
        "interpolation": Infinity,
        "extrapolation": Infinity
      },
      "stdout": "running\n  - layer_type: -1\n  - layer_type: DAG\n  - first_layer: None\n  - operation: div\n  - num_subsets: 1\n  - regualizer: 100.0\n  - regualizer_z: 1.0\n  - regualizer_oob: 1.0\n  -\n  - max_iterations: 2000\n  - batch_size: 64\n  - seed: 7064\n  -\n  - interpolation_range: [1, 2]\n  - extrapolation_range: [2, 4]\n  - input_size: 2\n  - output_size: 1\n  - subset_ratio: 0.5\n  - overlap_ratio: 0.0\n  - simple: False\n  -\n  - hidden_size: 1\n  - nac_mul: none\n  - oob_mode: clip\n  - regualizer_scaling: linear\n  - regualizer_scaling_start: 1000000\n  - regualizer_scaling_end: 2000000\n  - regualizer_shape: linear\n  - mnac_epsilon: 0\n  - nalu_bias: False\n  - nalu_two_nac: False\n  - nalu_two_gate: False\n  - nalu_mul: normal\n  - nalu_gate: normal\n  - nac_weight: normal\n  -\n  - optimizer: adam\n  - learning_rate: 5e-05\n  - momentum: 0.0\n  -\n  - cuda: False\n  - name_prefix: simple_function_static\n  - remove_existing_data: False\n  - verbose: False\n  -\n  - reg_scale_type: heim\n  - regualizer_beta_start: 1e-05\n  - regualizer_beta_end: 0.0001\n  - regualizer_beta_step: 10000\n  - regualizer_beta_growth: 10\n  - regualizer_l1: False\n  - regualizer-npu-w: 0\n  - regualizer-gate: 0\n  - npu-clip: none\n  - npu-Wr-init: xavier-uniform\n  -\n  - pytorch-precision: torch.float32\n  -\n  - no-save: False\n  - load-checkpoint: False\n  - log-interval: 500\n  -\n  - clip-grad-norm: 0.1\n  - nru_div_mode: div\n  - realnpu_reg_type: W\n  -\n  - reinit: False\n  - reinit_epoch_interval: 10\n  - reinit_max_stored_losses: 5000\n  - reinit_loss_thr: 1.0\n  - num_bins: 5\n  -\n  -\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - hypersearch_1\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/zwwc5925\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_174249-zwwc5925/logs\u001b[0m\n",
      "stderr": "c5925\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 777, in <module>\n    print(f\"  - dataset: {dataset.print_operation()}\")\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 130, in print_operation\n    return getattr(ARITHMETIC_FUNCTIONS_STRINGIY, self._operation_name)(*subset_str)\nTypeError: ARITHMETIC_FUNCTIONS_STRINGIY.div() missing 1 required positional argument: 'b'\n"
    },
    {
      "experiment_id": 2,
      "config": {
        "learning_rate": 0.0002,
        "clip_grad_norm": 1.0,
        "regualizer": 10.0,
        "regualizer_z": 0.0,
        "regualizer_oob": 0.1,
        "hidden_size": 4,
        "batch_size": 256,
        "optimizer": "sgd",
        "momentum": 0.9,
        "num_subsets": 3,
        "max_target_magnitude": 1000.0,
        "div_regularizer": null
      },
      "success": false,
      "grokked": false,
      "early_stopped": false,
      "duration": 7.307599067687988,
      "final_losses": {
        "train": Infinity,
        "interpolation": Infinity,
        "extrapolation": Infinity
      },
      "stdout": "running\n  - layer_type: -1\n  - layer_type: DAG\n  - first_layer: None\n  - operation: div\n  - num_subsets: 3\n  - regualizer: 10.0\n  - regualizer_z: 0.0\n  - regualizer_oob: 0.1\n  -\n  - max_iterations: 2000\n  - batch_size: 256\n  - seed: 2925\n  -\n  - interpolation_range: [1, 2]\n  - extrapolation_range: [2, 4]\n  - input_size: 2\n  - output_size: 1\n  - subset_ratio: 0.5\n  - overlap_ratio: 0.0\n  - simple: False\n  -\n  - hidden_size: 4\n  - nac_mul: none\n  - oob_mode: clip\n  - regualizer_scaling: linear\n  - regualizer_scaling_start: 1000000\n  - regualizer_scaling_end: 2000000\n  - regualizer_shape: linear\n  - mnac_epsilon: 0\n  - nalu_bias: False\n  - nalu_two_nac: False\n  - nalu_two_gate: False\n  - nalu_mul: normal\n  - nalu_gate: normal\n  - nac_weight: normal\n  -\n  - optimizer: sgd\n  - learning_rate: 0.0002\n  - momentum: 0.9\n  -\n  - cuda: False\n  - name_prefix: simple_function_static\n  - remove_existing_data: False\n  - verbose: False\n  -\n  - reg_scale_type: heim\n  - regualizer_beta_start: 1e-05\n  - regualizer_beta_end: 0.0001\n  - regualizer_beta_step: 10000\n  - regualizer_beta_growth: 10\n  - regualizer_l1: False\n  - regualizer-npu-w: 0\n  - regualizer-gate: 0\n  - npu-clip: none\n  - npu-Wr-init: xavier-uniform\n  -\n  - pytorch-precision: torch.float32\n  -\n  - no-save: False\n  - load-checkpoint: False\n  - log-interval: 500\n  -\n  - clip-grad-norm: 1.0\n  - nru_div_mode: div\n  - realnpu_reg_type: W\n  -\n  - reinit: False\n  - reinit_epoch_interval: 10\n  - reinit_max_stored_losses: 5000\n  - reinit_loss_thr: 1.0\n  - num_bins: 5\n  -\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - hypersearch_2\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/vml3p1qh\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_174257-vml3p1qh/logs\u001b[0m\n",
      "stderr": "dataset/simple_function_static.py\", line 8, in __init__\n    super().__init__(operation, input_size, **kwargs)\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 121, in __init__\n    raise ValueError(\"too many subsets given the subset and overlap ratios\")\nValueError: too many subsets given the subset and overlap ratios\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 765, in <module>\n    dataset = stable_nalu.dataset.SimpleFunctionStaticDataset(\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/simple_function_static.py\", line 8, in __init__\n    super().__init__(operation, input_size, **kwargs)\n  File \"/Users/paul_curry/ai2/nalm-benchmark/stable_nalu/dataset/_simple_function_abstact.py\", line 121, in __init__\n    raise ValueError(\"too many subsets given the subset and overlap ratios\")\nValueError: too many subsets given the subset and overlap ratios\n"
    }
  ],
  "grokked_configs": [],
  "summary": {
    "total_experiments": 3,
    "successful_experiments": 1,
    "grokking_experiments": 0,
    "grokking_rate": 0.0
  }
}