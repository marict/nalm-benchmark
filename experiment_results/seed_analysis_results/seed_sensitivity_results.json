{
  "metadata": {
    "operations": [
      "add",
      "sub",
      "mul",
      "div"
    ],
    "seeds": [
      1,
      7,
      42,
      99,
      123,
      221,
      234,
      345,
      456,
      789
    ],
    "max_iterations": 2000,
    "hyperparameters": {
      "layer_type": "DAG",
      "input_size": 2,
      "batch_size": 512,
      "max_iterations": 2000,
      "learning_rate": 0.01,
      "interpolation_range": "[-2.0,2.0]",
      "extrapolation_range": "[[-6.0,-2.0],[2.0,6.0]]",
      "no_cuda": true,
      "log_interval": 100,
      "clip_grad_norm": 0.01,
      "no_open_browser": true
    },
    "grokking_threshold": 1e-08
  },
  "results": [
    {
      "operation": "add",
      "seed": 7,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 170,
      "duration": 22.236414909362793,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -2.50159\n    Step 2:\n        selectors: [-0.0, -0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, -2.50159, 0.0]\n        G: 0.0 \u2192 computed_value: -2.50159\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.92057, 3.59711, -2.16112]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [3.37315, -2.50159, -2.50159]\n\tselected_value (hardened eval): -2.50159\nSample statistics (SOFT training state):\ninput=[1.93651, 1.8975]\noutput=3.89285, target=3.83401\nG (soft training): [0.97976, 0.0433, 0.99683]\n    Step 0:\n        selectors: [1.61924, 0.14347, -0.0, -0.0, -0.0]\n        inputs:    [-5.87474, 3.37315, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.37315\n    Step 1:\n        selectors: [-0.01454, -0.35208, 0.46332, 0.0, -0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -2.50159\n    Step 2:\n        selectors: [1.44328, 1.32174, -0.39177, 0.00175, -0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, -2.50159, 0.0]\n        G: 1.0 \u2192 computed_value: -2.50159\nOutput Selector (soft training):\n\tlogits (soft training): [1.69164, -2.8559, 2.90034]\n\tselected_node: 2\n\tintermediate_values (soft training): [3.87851, 1.47953, 3.90477]\n\tselected_value (soft training): 3.90477\nEarly stopped at step 170\nfinished:\n  - loss_train_capped: 4.6247740575035886e-15\n  - loss_train (+reg loss): 0.0003791885683313012\n  - loss_train_criterion: 0.0003791885683313012\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 170 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ae81h3zf\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-ae81h3zf/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-ae81h3zf\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ae81h3zf\n"
    },
    {
      "operation": "add",
      "seed": 99,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 178,
      "duration": 22.097729921340942,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "e: -6.40207\n    Step 1:\n        selectors: [0.0, -0.0, -0.0, 0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 1.0, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.58899, -0.72341, -2.46922]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-6.40207, 1.0, 1.0]\n\tselected_value (hardened eval): -6.40207\nSample statistics (SOFT training state):\ninput=[1.92771, 0.21161]\noutput=2.13512, target=2.13932\nG (soft training): [0.99627, 0.20222, 0.03311]\n    Step 0:\n        selectors: [1.009, 1.03938, -0.0, -0.0, 0.0]\n        inputs:    [-2.39794, -4.00413, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -6.40207\n    Step 1:\n        selectors: [0.43925, -0.11377, -0.25428, 0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.06407, -0.20769, -0.00696, 0.94065, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [2.58899, -0.72341, -2.46922]\n\tselected_node: 0\n\tintermediate_values (soft training): [2.17748, 1.09654, 1.42342]\n\tselected_value (soft training): 2.17748\nEarly stopped at step 178\nfinished:\n  - loss_train_capped: 4.6160822289153386e-15\n  - loss_train (+reg loss): 0.0005652736872434616\n  - loss_train_criterion: 0.0005652736872434616\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 178 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g91bzoti\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-g91bzoti/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-g91bzoti\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g91bzoti\n"
    },
    {
      "operation": "add",
      "seed": 1,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 183,
      "duration": 23.468570232391357,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "126\n    Step 1:\n        selectors: [0.0, -0.0, 0.0, 0.0, -0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.82863\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.57296, -3.02173, 3.7646]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-2.27126, 1.0, -5.82863]\n\tselected_value (hardened eval): -5.82863\nSample statistics (SOFT training state):\ninput=[-0.88516, 1.72836]\noutput=0.83183, target=0.84319\nG (soft training): [0.43487, 0.04549, 0.99304]\n    Step 0:\n        selectors: [0.28801, 1.30187, -0.0, 0.0, -0.0]\n        inputs:    [-3.55737, -2.27126, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: -2.27126\n    Step 1:\n        selectors: [0.15913, -0.02203, 0.08013, 0.0, -0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.01048, 1.07934, -0.0633, -0.02881, 0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.82863\nOutput Selector (soft training):\n\tlogits (soft training): [-0.57296, -3.02173, 3.7646]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.97334, 0.86598, 0.81688]\n\tselected_value (soft training): 0.81688\nEarly stopped at step 183\nfinished:\n  - loss_train_capped: 1.113855101902983e-14\n  - loss_train (+reg loss): 0.0002933297655545175\n  - loss_train_criterion: 0.0002933297655545175\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 183 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/aampgqsw\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-aampgqsw/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-aampgqsw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/aampgqsw\n"
    },
    {
      "operation": "add",
      "seed": 123,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 152,
      "duration": 20.882658004760742,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "1.0\n    Step 1:\n        selectors: [0.0, 1.0, 0.0, 0.0, 0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -2.74181\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, -0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, -2.74181, 0.0]\n        G: 1.0 \u2192 computed_value: -7.32969\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.54113, 0.41014, 3.18632]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, -2.74181, -7.32969]\n\tselected_value (hardened eval): -7.32969\nSample statistics (SOFT training state):\ninput=[0.90836, -0.05174]\noutput=0.84462, target=0.85663\nG (soft training): [0.99675, 0.68358, 0.03752]\n    Step 0:\n        selectors: [1.00911, 1.03136, 0.0, 0.0, -0.0]\n        inputs:    [-4.58788, -2.74181, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.57229, -0.12865, -0.37202, -0.0, -0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, 0.0, 0.0]\n        G: 0.7 \u2192 computed_value: -2.74181\n    Step 2:\n        selectors: [-0.10955, -0.18181, -0.04278, 1.15736, 0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, -2.74181, 0.0]\n        G: 0.0 \u2192 computed_value: -7.32969\nOutput Selector (soft training):\n\tlogits (soft training): [2.76275, -1.39271, -2.51112]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.84952, 0.57865, 0.70153]\n\tselected_value (soft training): 0.84952\nEarly stopped at step 152\nfinished:\n  - loss_train_capped: 3.903036764905986e-15\n  - loss_train (+reg loss): 0.00040152965812012553\n  - loss_train_criterion: 0.00040152965812012553\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 152 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:39\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/cp21pnfx\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233839-cp21pnfx/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233839-cp21pnfx\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:39\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/cp21pnfx\n"
    },
    {
      "operation": "add",
      "seed": 221,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 178,
      "duration": 21.878474235534668,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "e: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, -0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.60618\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 1.60618, 0.0]\n        G: 1.0 \u2192 computed_value: -6.40207\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-3.47604, 0.17633, 3.18269]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.60618, -6.40207]\n\tselected_value (hardened eval): -6.40207\nSample statistics (SOFT training state):\ninput=[-0.66813, -1.94609]\noutput=-2.61897, target=-2.61421\nG (soft training): [0.02552, 0.96754, 0.99657]\n    Step 0:\n        selectors: [0.03036, -0.06161, 0.0, -0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.27176, -0.62854, 0.16474, -0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.60618\n    Step 2:\n        selectors: [1.12249, 1.02943, -0.01896, -0.093, 0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 1.60618, 0.0]\n        G: 1.0 \u2192 computed_value: -6.40207\nOutput Selector (soft training):\n\tlogits (soft training): [-3.47604, 0.17633, 3.18269]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.84389, 0.50117, -2.77777]\n\tselected_value (soft training): -2.77777\nEarly stopped at step 178\nfinished:\n  - loss_train_capped: 4.895381178877077e-15\n  - loss_train (+reg loss): 0.0008148237247951329\n  - loss_train_criterion: 0.0008148237247951329\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 178 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:39\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/54q0t3jc\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233839-54q0t3jc/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233839-54q0t3jc\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:39\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/54q0t3jc\n"
    },
    {
      "operation": "add",
      "seed": 234,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 246,
      "duration": 24.207001209259033,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": ".0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.76581\n    Step 2:\n        selectors: [1.0, -0.0, 1.0, 0.0, 0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 1.76581, 0.0]\n        G: 1.0 \u2192 computed_value: -2.51491\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.12249, 3.3987, -1.65079]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 1.76581, -2.51491]\n\tselected_value (hardened eval): 1.76581\nSample statistics (SOFT training state):\ninput=[0.89154, 0.62703]\noutput=1.5372, target=1.51857\nG (soft training): [0.99928, 0.01824, 0.00192]\n    Step 0:\n        selectors: [1.76174, 1.00974, 0.0, 0.0, 0.0]\n        inputs:    [-3.51491, 5.28072, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.08075, -0.4602, 0.64955, 0.0, -0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.76581\n    Step 2:\n        selectors: [-0.0055, 1.02214, -7e-05, -0.00125, -0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 1.76581, 0.0]\n        G: 0.0 \u2192 computed_value: -2.51491\nOutput Selector (soft training):\n\tlogits (soft training): [2.35447, -3.6095, 2.04058]\n\tselected_node: 0\n\tintermediate_values (soft training): [2.20663, 2.0801, 0.61903]\n\tselected_value (soft training): 2.20663\nEarly stopped at step 246\nfinished:\n  - loss_train_capped: 4.786864823389013e-15\n  - loss_train (+reg loss): 0.00046885700430721045\n  - loss_train_criterion: 0.00046885700430721045\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 246 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:40\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v1dgevje\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233840-v1dgevje/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233840-v1dgevje\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:40\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v1dgevje\n"
    },
    {
      "operation": "add",
      "seed": 456,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 168,
      "duration": 21.401931047439575,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "  Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, 0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.37101\n    Step 2:\n        selectors: [-0.0, -0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.37101, 0.0]\n        G: 0.0 \u2192 computed_value: 0.37101\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.87509, 4.40768, -2.55521]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [5.55641, 0.37101, 0.37101]\n\tselected_value (hardened eval): 0.37101\nSample statistics (SOFT training state):\ninput=[1.51957, 0.40007]\noutput=1.89094, target=1.91964\nG (soft training): [0.43375, 0.03312, 0.99601]\n    Step 0:\n        selectors: [1.42894, -0.01468, 0.0, 0.0, 0.0]\n        inputs:    [-5.18539, 5.55641, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: 5.55641\n    Step 1:\n        selectors: [-0.10523, -0.1206, 1.01706, -0.0, -0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.37101\n    Step 2:\n        selectors: [1.06654, 1.03246, -0.09073, 0.01997, -0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.37101, 0.0]\n        G: 1.0 \u2192 computed_value: 0.37101\nOutput Selector (soft training):\n\tlogits (soft training): [-0.16801, -2.49441, 3.22347]\n\tselected_node: 2\n\tintermediate_values (soft training): [2.60532, 2.90244, 1.86357]\n\tselected_value (soft training): 1.86357\nEarly stopped at step 168\nfinished:\n  - loss_train_capped: 4.544341502898215e-15\n  - loss_train (+reg loss): 0.0005102852592244744\n  - loss_train_criterion: 0.0005102852592244744\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 168 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:39:00\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/8pkbewq2\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233900-8pkbewq2/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233900-8pkbewq2\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:39:00\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/8pkbewq2\n"
    },
    {
      "operation": "add",
      "seed": 345,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 196,
      "duration": 22.906141996383667,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": ":\n        selectors: [1.0, 0.0, 0.0, 0.0, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.62527\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 3.62527, 0.0]\n        G: 1.0 \u2192 computed_value: 0.40106\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.52722, -0.78557, 2.18211]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-3.22421, 3.62527, 0.40106]\n\tselected_value (hardened eval): 0.40106\nSample statistics (SOFT training state):\ninput=[0.97654, -0.26238]\noutput=0.71944, target=0.71416\nG (soft training): [0.03862, 0.94282, 0.98832]\n    Step 0:\n        selectors: [0.17658, 0.54487, -0.0, 0.0, -0.0]\n        inputs:    [3.62527, -3.22421, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -3.22421\n    Step 1:\n        selectors: [1.355, 0.13753, 0.19357, 0.0, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 3.62527\n    Step 2:\n        selectors: [1.34589, 1.12329, -0.08998, -0.23765, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 3.62527, 0.0]\n        G: 1.0 \u2192 computed_value: 0.40106\nOutput Selector (soft training):\n\tlogits (soft training): [-2.52722, -0.78557, 2.18211]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.08733, 1.27404, 0.69819]\n\tselected_value (soft training): 0.69819\nEarly stopped at step 196\nfinished:\n  - loss_train_capped: 4.9061512028014155e-15\n  - loss_train (+reg loss): 0.0003634786990005523\n  - loss_train_criterion: 0.0003634786990005523\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 196 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:39:00\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u4xfu32k\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233900-u4xfu32k/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233900-u4xfu32k\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:39:00\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u4xfu32k\n"
    },
    {
      "operation": "add",
      "seed": 789,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 191,
      "duration": 21.57179617881775,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "   G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, 0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: -10.31961\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-4.2421, -2.18177, 3.72415]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.0, -10.31961]\n\tselected_value (hardened eval): -10.31961\nSample statistics (SOFT training state):\ninput=[1.77139, -1.29738]\noutput=0.49226, target=0.47401\nG (soft training): [0.9975, 0.97876, 0.04194]\n    Step 0:\n        selectors: [0.99198, 1.04408, -0.0, 0.0, -0.0]\n        inputs:    [-5.69853, -4.62108, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.44866, -0.31764, 0.01559, 0.0, -0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [0.0268, -0.35241, 0.35296, 0.39135, 0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: -10.31961\nOutput Selector (soft training):\n\tlogits (soft training): [4.16015, 0.71575, -2.87142]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.40443, 3.24294, 0.51683]\n\tselected_value (soft training): 0.40443\nEarly stopped at step 191\nfinished:\n  - loss_train_capped: 4.423142099574439e-15\n  - loss_train (+reg loss): 0.0005646094214171171\n  - loss_train_criterion: 0.0005646094214171171\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 191 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:39:04\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/q8m7sk42\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233904-q8m7sk42/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233904-q8m7sk42\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:39:04\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/q8m7sk42\n"
    },
    {
      "operation": "sub",
      "seed": 1,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 282,
      "duration": 26.8727810382843,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": " \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [2.3799, 3.27348, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.89358\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 1.0, 0.0]\n        inputs:    [2.3799, 3.27348, 1.0, -0.89358, 0.0]\n        G: 1.0 \u2192 computed_value: -1.78716\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.33842, 2.30904, -0.37735]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, -0.89358, -1.78716]\n\tselected_value (hardened eval): -0.89358\nSample statistics (SOFT training state):\ninput=[1.04785, 0.11851]\noutput=1.0006, target=0.92934\nG (soft training): [0.97773, 0.25195, 0.02982]\n    Step 0:\n        selectors: [0.96917, -0.97634, 0.0, -0.0, 0.0]\n        inputs:    [2.3799, 3.27348, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.08226, -0.04288, 0.30162, -0.0, 0.0]\n        inputs:    [2.3799, 3.27348, 1.0, 0.0, 0.0]\n        G: 0.3 \u2192 computed_value: -0.89358\n    Step 2:\n        selectors: [-0.06809, 0.00015, 1.08751, -0.57778, -0.0]\n        inputs:    [2.3799, 3.27348, 1.0, -0.89358, 0.0]\n        G: 0.0 \u2192 computed_value: -1.78716\nOutput Selector (soft training):\n\tlogits (soft training): [0.62972, -2.40162, 1.1386]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.96283, 0.85966, 1.02739]\n\tselected_value (soft training): 1.02739\nEarly stopped at step 282\nfinished:\n  - loss_train_capped: 4.746454999057879e-15\n  - loss_train (+reg loss): 0.0009271769085898995\n  - loss_train_criterion: 0.0009271769085898995\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 282 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:22\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/n8hbv5vh\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233922-n8hbv5vh/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233922-n8hbv5vh\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:22\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/n8hbv5vh\n"
    },
    {
      "operation": "sub",
      "seed": 7,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 316,
      "duration": 28.284199953079224,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, 0.0, 0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.62717\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 1.0, 0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, -8.62717, 0.0]\n        G: 1.0 \u2192 computed_value: -17.25434\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.87341, 2.72882, -1.03841]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, -8.62717, -17.25434]\n\tselected_value (hardened eval): -8.62717\nSample statistics (SOFT training state):\ninput=[1.76364, -1.92693]\noutput=3.90232, target=3.69057\nG (soft training): [0.97547, 0.15669, 0.01305]\n    Step 0:\n        selectors: [0.95335, -0.95904, -0.0, -0.0, -0.0]\n        inputs:    [-4.9646, 3.66257, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.04071, 6e-05, -0.5031, 0.0, -0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: -8.62717\n    Step 2:\n        selectors: [-0.04118, -0.00016, 0.89467, -0.15391, -0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, -8.62717, 0.0]\n        G: 0.0 \u2192 computed_value: -17.25434\nOutput Selector (soft training):\n\tlogits (soft training): [0.18374, -2.97524, 2.44829]\n\tselected_node: 2\n\tintermediate_values (soft training): [3.92031, 0.32694, 3.91622]\n\tselected_value (soft training): 3.91622\nEarly stopped at step 316\nfinished:\n  - loss_train_capped: 4.2807833897117765e-15\n  - loss_train (+reg loss): 0.0014130872441455722\n  - loss_train_criterion: 0.0014130872441455722\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 316 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:22\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/z8ks77az\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233922-z8ks77az/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233922-z8ks77az\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:22\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/z8ks77az\n"
    },
    {
      "operation": "sub",
      "seed": 42,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 335,
      "duration": 29.424649000167847,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": " 1:\n        selectors: [1.0, -1.0, 1.0, 0.0, -0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.25349\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, 0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, -0.25349, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.65279, -0.42184, -2.17075]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-0.12675, -0.25349, 1.0]\n\tselected_value (hardened eval): -0.12675\nSample statistics (SOFT training state):\ninput=[1.0411, 1.69121]\noutput=-0.66495, target=-0.65012\nG (soft training): [0.98991, 0.93221, 0.08219]\n    Step 0:\n        selectors: [1.01757, -1.00523, 0.0, 0.0, 0.0]\n        inputs:    [-5.57697, -5.45023, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.12675\n    Step 1:\n        selectors: [0.86234, -1.25667, 0.82478, 0.0, -0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -0.25349\n    Step 2:\n        selectors: [-0.09446, 0.10698, 0.38693, 0.25218, 0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, -0.25349, 0.0]\n        G: 0.1 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [2.65279, -0.42184, -2.17075]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.62512, -1.57279, -0.40074]\n\tselected_value (soft training): -0.62512\nEarly stopped at step 335\nfinished:\n  - loss_train_capped: 5.13115025864647e-15\n  - loss_train (+reg loss): 0.0005899915704503655\n  - loss_train_criterion: 0.0005899915704503655\n  - loss_valid_inter: 1.338404484896999e-13\n  - loss_valid_extra: 7.363674150691682e-14\n\nModel (/checkpoint)  trained for 335 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:25\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/d3ku7pix\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233926-d3ku7pix/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233926-d3ku7pix\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:25\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/d3ku7pix\n"
    },
    {
      "operation": "add",
      "seed": 42,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 113.33402705192566,
      "final_inter_loss": 0.6720571518,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 5.14657\n    Step 2:\n        selectors: [1.0, 1.0, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 5.14657, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-4.08727, 3.10393, 2.56144]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 5.14657, 9.62063]\n\tselected_value (hardened eval): 5.14657\nSample statistics (SOFT training state):\ninput=[-0.57972, 0.83151]\noutput=0.25323, target=0.25179\nG (soft training): [0.00405, 0.99952, 0.99937]\n    Step 0:\n        selectors: [0.09854, 0.02211, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.4943, 1.40287, -0.02138, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 5.14657\n    Step 2:\n        selectors: [2.04458, 0.84977, 0.02434, -0.39078, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 5.14657, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\nOutput Selector (soft training):\n\tlogits (soft training): [-4.08727, 3.10393, 2.56144]\n\tselected_node: 1\n\tintermediate_values (soft training): [0.8882, 0.86059, -0.79244]\n\tselected_value (soft training): 0.86059\nfinished:\n  - loss_train_capped: 0.6496227383613586\n  - loss_train (+reg loss): 4.824825009563938e-05\n  - loss_train_criterion: 4.824825009563938e-05\n  - loss_valid_inter: 0.6720571517944336\n  - loss_valid_extra: 8.708630561828613\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/clj25y4w\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-clj25y4w/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-clj25y4w\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/clj25y4w\n"
    },
    {
      "operation": "sub",
      "seed": 99,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 300,
      "duration": 27.474493980407715,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "uted_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, -0.0, 0.0, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.00808\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 1.0, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 3.00808, 0.0]\n        G: 0.0 \u2192 computed_value: 3.00808\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.3289, 2.71322, -0.11402]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 3.00808, 3.00808]\n\tselected_value (hardened eval): 3.00808\nSample statistics (SOFT training state):\ninput=[0.04434, -1.71667]\noutput=1.77771, target=1.761\nG (soft training): [0.0119, 0.99404, 0.02227]\n    Step 0:\n        selectors: [-0.13263, 0.14676, -0.0, -0.0, 0.0]\n        inputs:    [-2.82254, -5.83063, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.04918, -1.04204, -0.04204, 0.0, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.00808\n    Step 2:\n        selectors: [-0.19002, -0.007, -0.16923, 0.82185, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 3.00808, 0.0]\n        G: 0.0 \u2192 computed_value: 3.00808\nOutput Selector (soft training):\n\tlogits (soft training): [-0.3289, 2.71322, -0.11402]\n\tselected_node: 1\n\tintermediate_values (soft training): [1.44246, 1.74384, 2.62044]\n\tselected_value (soft training): 1.74384\nEarly stopped at step 300\nfinished:\n  - loss_train_capped: 4.585487398860513e-15\n  - loss_train (+reg loss): 0.0004334343830123544\n  - loss_train_criterion: 0.0004334343830123544\n  - loss_valid_inter: 1.338404484896999e-13\n  - loss_valid_extra: 7.363674150691682e-14\n\nModel (/checkpoint)  trained for 300 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:49\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/132052i6\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233949-132052i6/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233949-132052i6\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:49\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/132052i6\n"
    },
    {
      "operation": "sub",
      "seed": 123,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 334,
      "duration": 29.450573921203613,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "puted_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, -0.0, -0.0, -0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 8.41028\n    Step 2:\n        selectors: [-0.0, 0.0, -0.0, 1.0, 0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 8.41028, 0.0]\n        G: 0.0 \u2192 computed_value: 8.41028\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.47842, 0.27979, 2.12992]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 8.41028, 8.41028]\n\tselected_value (hardened eval): 8.41028\nSample statistics (SOFT training state):\ninput=[1.62067, 1.78536]\noutput=-0.14275, target=-0.16469\nG (soft training): [0.98318, 0.11577, 0.965]\n    Step 0:\n        selectors: [1.13978, -1.08063, -0.0, -0.0, 0.0]\n        inputs:    [4.87427, -3.53601, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.1487, 0.14661, 1.88838, 0.0, 0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 0.0, 0.0]\n        G: 0.1 \u2192 computed_value: 8.41028\n    Step 2:\n        selectors: [0.40507, -1.45034, 0.36195, -0.18034, -0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 8.41028, 0.0]\n        G: 1.0 \u2192 computed_value: 8.41028\nOutput Selector (soft training):\n\tlogits (soft training): [2.09373, -0.99186, -1.08245]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.09291, 0.32042, -1.84388]\n\tselected_value (soft training): -0.09291\nEarly stopped at step 334\nfinished:\n  - loss_train_capped: 5.341872574165553e-15\n  - loss_train (+reg loss): 0.0009477154235355556\n  - loss_train_criterion: 0.0009477154235355556\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 334 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:51\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6fqvr2oy\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233951-6fqvr2oy/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233951-6fqvr2oy\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:51\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6fqvr2oy\n"
    },
    {
      "operation": "sub",
      "seed": 221,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 422,
      "duration": 33.4984347820282,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": ": 1.0\n    Step 1:\n        selectors: [1.0, -1.0, -0.0, 0.0, 0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, -0.0, -0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, -8.35478, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.93495, -2.37999, 2.44857]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, -8.35478, -8.35478]\n\tselected_value (hardened eval): -8.35478\nSample statistics (SOFT training state):\ninput=[-0.16492, 0.64124]\noutput=-0.82366, target=-0.80616\nG (soft training): [0.23355, 0.97864, 0.98882]\n    Step 0:\n        selectors: [0.05815, 0.06822, -0.0, 0.0, 0.0]\n        inputs:    [-2.67106, 5.68373, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.00154, -0.75376, -0.34887, 0.0, 0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\n    Step 2:\n        selectors: [1.22009, -1.18193, -0.067, -0.1813, -0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, -8.35478, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\nOutput Selector (soft training):\n\tlogits (soft training): [-1.93495, -2.37999, 2.44857]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.37393, -0.78345, -0.83893]\n\tselected_value (soft training): -0.83893\nEarly stopped at step 422\nfinished:\n  - loss_train_capped: 4.346448771914719e-15\n  - loss_train (+reg loss): 0.0005895174108445644\n  - loss_train_criterion: 0.0005895174108445644\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 422 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:55\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ksogwj81\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233955-ksogwj81/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233955-ksogwj81\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:55\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ksogwj81\n"
    },
    {
      "operation": "sub",
      "seed": 456,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 290,
      "duration": 27.390222787857056,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "6.90475\n    Step 1:\n        selectors: [-0.0, -0.0, 1.0, -0.0, -0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 6.90475\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, -0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 6.90475, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.73863, 2.70559, -1.67081]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [6.90475, 6.90475, 1.0]\n\tselected_value (hardened eval): 6.90475\nSample statistics (SOFT training state):\ninput=[0.36515, 0.55986]\noutput=-0.19925, target=-0.1947\nG (soft training): [0.22321, 0.968, 0.98699]\n    Step 0:\n        selectors: [-0.02649, 0.20779, -0.0, -0.0, -0.0]\n        inputs:    [3.26765, -3.6371, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 6.90475\n    Step 1:\n        selectors: [1.30747, -1.06975, -0.03578, 0.0, 0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 6.90475\n    Step 2:\n        selectors: [1.25142, -1.22165, -0.00185, -0.15814, 0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 6.90475, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [-2.07113, -2.09412, 2.57976]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.41549, -0.16516, -0.20544]\n\tselected_value (soft training): -0.20544\nEarly stopped at step 290\nfinished:\n  - loss_train_capped: 3.948637207137895e-15\n  - loss_train (+reg loss): 0.0022183540277183056\n  - loss_train_criterion: 0.0022183540277183056\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 290 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:20\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/2bes8wu3\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234020-2bes8wu3/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234020-2bes8wu3\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:20\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/2bes8wu3\n"
    },
    {
      "operation": "sub",
      "seed": 345,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 428,
      "duration": 34.53119421005249,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "94\n    Step 1:\n        selectors: [0.0, -0.0, 1.0, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\n    Step 2:\n        selectors: [-0.0, -0.0, 1.0, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.98594, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.15782, -0.40129, 0.79163]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.98594, 0.98594, 0.98594]\n\tselected_value (hardened eval): 0.98594\nSample statistics (SOFT training state):\ninput=[-0.66762, -1.75979]\noutput=1.0831, target=1.09217\nG (soft training): [0.9937, 0.00906, 0.00532]\n    Step 0:\n        selectors: [0.98312, -0.99229, -0.0, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.98594\n    Step 1:\n        selectors: [0.02927, -0.00312, 0.8711, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\n    Step 2:\n        selectors: [-0.03766, -0.00035, 0.58402, 0.46002, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.98594, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\nOutput Selector (soft training):\n\tlogits (soft training): [-0.15782, -0.40129, 0.79163]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.08866, 1.0562, 1.0891]\n\tselected_value (soft training): 1.0891\nEarly stopped at step 428\nfinished:\n  - loss_train_capped: 3.615620274694236e-15\n  - loss_train (+reg loss): 0.0006584612419828773\n  - loss_train_criterion: 0.0006584612419828773\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 428 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:16\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u10tnkfd\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234016-u10tnkfd/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234016-u10tnkfd\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:16\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u10tnkfd\n"
    },
    {
      "operation": "sub",
      "seed": 789,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 441,
      "duration": 34.702476024627686,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": " selectors: [-0.0, 0.0, 1.0, -0.0, 0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -8.30591\n    Step 2:\n        selectors: [1.0, -1.0, 1.0, -0.0, -0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, -8.30591, 0.0]\n        G: 1.0 \u2192 computed_value: -16.61181\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.15201, -1.1827, -0.756]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-8.30591, -8.30591, -16.61181]\n\tselected_value (hardened eval): -8.30591\nSample statistics (SOFT training state):\ninput=[-1.89626, 1.91763]\noutput=-3.78123, target=-3.81389\nG (soft training): [0.98681, 0.19733, 0.97716]\n    Step 0:\n        selectors: [1.02058, -1.00875, 0.0, -0.0, 0.0]\n        inputs:    [-4.62097, 3.68493, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.30591\n    Step 1:\n        selectors: [-0.10476, 0.10452, 0.99907, -0.0, 0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: -8.30591\n    Step 2:\n        selectors: [0.60862, -1.08724, 0.93549, -0.26602, -0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, -8.30591, 0.0]\n        G: 1.0 \u2192 computed_value: -16.61181\nOutput Selector (soft training):\n\tlogits (soft training): [2.15201, -1.1827, -0.756]\n\tselected_node: 0\n\tintermediate_values (soft training): [-3.76852, -0.71907, -6.01257]\n\tselected_value (soft training): -3.76852\nEarly stopped at step 441\nfinished:\n  - loss_train_capped: 4.60367404327101e-15\n  - loss_train (+reg loss): 0.00120429671369493\n  - loss_train_criterion: 0.00120429671369493\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 441 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:28\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/au8ykvnr\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234029-au8ykvnr/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234029-au8ykvnr\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:28\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/au8ykvnr\n"
    },
    {
      "operation": "mul",
      "seed": 1,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 159,
      "duration": 20.524561882019043,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "value: 0.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [0.0, 0.0, -0.0, 1.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -22.79159\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.12471, 2.06767, -1.94371]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, -22.79159, -22.79159]\n\tselected_value (hardened eval): -22.79159\nSample statistics (SOFT training state):\ninput=[0.42762, 0.16872]\noutput=0.10654, target=0.07215\nG (soft training): [0.01909, 0.93727, 0.01406]\n    Step 0:\n        selectors: [1.02841, 1.06505, 0.0, -0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.02925, -0.18103, -0.02014, -0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [0.00751, 0.20375, 1.01575, -0.26866, -0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, -22.79159, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\nOutput Selector (soft training):\n\tlogits (soft training): [0.70679, -1.2125, 1.38081]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.11279, -0.06864, 0.11646]\n\tselected_value (soft training): 0.11646\nEarly stopped at step 159\nfinished:\n  - loss_train_capped: 4.527506299555062e-15\n  - loss_train (+reg loss): 0.003092207945883274\n  - loss_train_criterion: 0.003092207945883274\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 159 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:40:47\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rpmmr9mi\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234048-rpmmr9mi/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234048-rpmmr9mi\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:40:47\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rpmmr9mi\n"
    },
    {
      "operation": "mul",
      "seed": 99,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 163,
      "duration": 20.409661054611206,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "0.0\n    Step 1:\n        selectors: [1.0, 0.0, -0.0, -0.0, 0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -3.99468\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, 0.0, 0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, -3.99468, 0.0]\n        G: 0.0 \u2192 computed_value: -14.18562\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.80659, -1.15112, 2.49592]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, -3.99468, -14.18562]\n\tselected_value (hardened eval): -14.18562\nSample statistics (SOFT training state):\ninput=[0.92211, 0.9089]\noutput=0.81928, target=0.83811\nG (soft training): [0.00652, 0.95038, 0.99417]\n    Step 0:\n        selectors: [0.99601, 1.01295, -0.0, -0.0, 0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.40487, 0.02697, 1.04134, 0.0, -0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -3.99468\n    Step 2:\n        selectors: [-1.03344, -0.19173, 1.28415, -0.11777, -0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, -3.99468, 0.0]\n        G: 1.0 \u2192 computed_value: -14.18562\nOutput Selector (soft training):\n\tlogits (soft training): [3.09937, -0.73985, -1.70554]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.83296, 0.54211, -0.12396]\n\tselected_value (soft training): 0.83296\nEarly stopped at step 163\nfinished:\n  - loss_train_capped: 4.9313690677070705e-15\n  - loss_train (+reg loss): 0.00036157373688183725\n  - loss_train_criterion: 0.00036157373688183725\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 163 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:08\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6c9o3q41\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234108-6c9o3q41/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234108-6c9o3q41\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:08\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6c9o3q41\n"
    },
    {
      "operation": "mul",
      "seed": 7,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 516,
      "duration": 38.54491996765137,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": " 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, -0.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 18.14457\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-3.75929, -0.76748, 3.59174]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, 1.0, 18.14457]\n\tselected_value (hardened eval): 18.14457\nSample statistics (SOFT training state):\ninput=[1.04053, -0.3848]\noutput=-0.40959, target=-0.40039\nG (soft training): [0.00286, 0.93055, 0.99719]\n    Step 0:\n        selectors: [1.02837, 1.00367, -0.0, -0.0, -0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-1.0577, -0.07917, 0.7205, 0.0, -0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-1.09287, -0.42293, 1.61124, 0.06445, -0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: 18.14457\nOutput Selector (soft training):\n\tlogits (soft training): [4.73994, -0.08669, -3.36985]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.40271, -1.22035, -1.68667]\n\tselected_value (soft training): -0.40271\nEarly stopped at step 516\nfinished:\n  - loss_train_capped: 3.599882402534251e-15\n  - loss_train (+reg loss): 0.00035132712218910456\n  - loss_train_criterion: 0.00035132712218910456\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 516 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:40:51\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/os4lbfu6\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234051-os4lbfu6/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234051-os4lbfu6\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:40:51\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/os4lbfu6\n"
    },
    {
      "operation": "mul",
      "seed": 123,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 161,
      "duration": 20.54447102546692,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "puted_value: 0.0\n    Step 1:\n        selectors: [0.0, -1.0, 1.0, 0.0, 0.0]\n        inputs:    [2.88672, 4.71099, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -4.71099\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [2.88672, 4.71099, 0.0, -4.71099, 0.0]\n        G: 0.0 \u2192 computed_value: 13.59931\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-3.19127, -1.3385, 3.37224]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, -4.71099, 13.59931]\n\tselected_value (hardened eval): 13.59931\nSample statistics (SOFT training state):\ninput=[-0.89555, -0.32758]\noutput=0.30409, target=0.29337\nG (soft training): [0.97681, 0.82121, 0.00686]\n    Step 0:\n        selectors: [-0.41278, -0.25771, -0.0, -0.0, 0.0]\n        inputs:    [2.88672, 4.71099, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.01433, -1.54208, 2.01782, 0.0, 0.0]\n        inputs:    [2.88672, 4.71099, 0.0, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: -4.71099\n    Step 2:\n        selectors: [1.02662, 1.00593, -0.01338, 0.0001, -0.0]\n        inputs:    [2.88672, 4.71099, 0.0, -4.71099, 0.0]\n        G: 0.0 \u2192 computed_value: 13.59931\nOutput Selector (soft training):\n\tlogits (soft training): [-3.19127, -1.3385, 3.37224]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.46052, 1.38681, 0.29413]\n\tselected_value (soft training): 0.29413\nEarly stopped at step 161\nfinished:\n  - loss_train_capped: 3.6281860084667536e-15\n  - loss_train (+reg loss): 0.0003029622894246131\n  - loss_train_criterion: 0.0003029622894246131\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 161 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:28\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ycdm4qkd\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234129-ycdm4qkd/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234129-ycdm4qkd\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:28\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ycdm4qkd\n"
    },
    {
      "operation": "mul",
      "seed": 221,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 159,
      "duration": 20.512435913085938,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": " selectors: [1.0, 1.0, 0.0, 0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [-0.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -4.9208\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.45805, 3.15884, -1.55981]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [-22.79159, -22.79159, -4.9208]\n\tselected_value (hardened eval): -22.79159\nSample statistics (SOFT training state):\ninput=[-1.59577, -1.54604]\noutput=2.46437, target=2.46711\nG (soft training): [0.02368, 0.00471, 0.96602]\n    Step 0:\n        selectors: [1.02244, 0.97808, -0.0, 0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 1:\n        selectors: [1.02128, 1.01136, 0.0171, 0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [-0.22668, -0.55154, 0.2855, 0.13321, -0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -4.9208\nOutput Selector (soft training):\n\tlogits (soft training): [-1.45805, 3.15884, -1.55981]\n\tselected_node: 1\n\tintermediate_values (soft training): [2.20969, 2.46953, 2.16809]\n\tselected_value (soft training): 2.46953\nEarly stopped at step 159\nfinished:\n  - loss_train_capped: 4.328910107708871e-15\n  - loss_train (+reg loss): 0.0007698783883824944\n  - loss_train_criterion: 0.0007698783883824944\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 159 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:29\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kwbfrhx\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234129-5kwbfrhx/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234129-5kwbfrhx\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:29\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kwbfrhx\n"
    },
    {
      "operation": "sub",
      "seed": 234,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 113.40524077415466,
      "final_inter_loss": 0.6413063407,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "omputed_value: -5.14657\n    Step 1:\n        selectors: [0.0, -1.0, 1.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -10.29313\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, -10.29313, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.72302, -1.17329, 1.1792]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-5.14657, -10.29313, -0.6725]\n\tselected_value (hardened eval): -0.6725\nSample statistics (SOFT training state):\ninput=[-1.41493, -1.46763]\noutput=0.00409, target=0.05271\nG (soft training): [0.96114, 0.0003, 0.00459]\n    Step 0:\n        selectors: [0.0, -2e-05, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [-0.00225, 0.00379, -1.03878, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -10.29313\n    Step 2:\n        selectors: [1.02565, 0.05174, -0.00154, -0.02281, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, -10.29313, 0.0]\n        G: 0.0 \u2192 computed_value: -0.6725\nOutput Selector (soft training):\n\tlogits (soft training): [-2.55567, 0.27934, 2.05017]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.00812, 7.53012, -1.27681]\n\tselected_value (soft training): -1.27681\nfinished:\n  - loss_train_capped: 0.6895342469215393\n  - loss_train (+reg loss): 0.0016467851819470525\n  - loss_train_criterion: 0.0016467851819470525\n  - loss_valid_inter: 0.6413062810897827\n  - loss_valid_extra: 8.613598823547363\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:10\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rzsfq5ak\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234010-rzsfq5ak/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234010-rzsfq5ak\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:10\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rzsfq5ak\n"
    },
    {
      "operation": "mul",
      "seed": 234,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 187,
      "duration": 21.60555076599121,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "d_value: 1.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 15.08698\n    Step 2:\n        selectors: [-0.0, -1.0, 0.0, 0.0, 0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 15.08698, 0.0]\n        G: 1.0 \u2192 computed_value: -5.92103\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.03851, 3.22703, -1.94603]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 15.08698, -5.92103]\n\tselected_value (hardened eval): 15.08698\nSample statistics (SOFT training state):\ninput=[0.39306, -1.10819]\noutput=-0.44327, target=-0.43558\nG (soft training): [0.49798, 0.98074, 0.00364]\n    Step 0:\n        selectors: [0.04764, 0.23163, 0.0, 0.0, 0.0]\n        inputs:    [2.54803, 5.92103, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.44267, -0.20585, -0.08891, 0.0, -0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 15.08698\n    Step 2:\n        selectors: [0.99961, 1.01226, -0.06017, -0.00135, -0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 15.08698, 0.0]\n        G: 0.0 \u2192 computed_value: -5.92103\nOutput Selector (soft training):\n\tlogits (soft training): [-0.88063, -1.73829, 3.291]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.06224, 0.08603, -0.45261]\n\tselected_value (soft training): -0.45261\nEarly stopped at step 187\nfinished:\n  - loss_train_capped: 3.68157195403335e-15\n  - loss_train (+reg loss): 8.781938231550157e-05\n  - loss_train_criterion: 8.781938231550157e-05\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 187 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:49\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g32myxhh\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234149-g32myxhh/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234149-g32myxhh\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:49\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g32myxhh\n"
    },
    {
      "operation": "mul",
      "seed": 345,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 182,
      "duration": 21.710609912872314,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "        selectors: [-1.0, -1.0, 0.0, -0.0, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.99502\n    Step 2:\n        selectors: [0.0, 0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.99502, 0.0]\n        G: 0.0 \u2192 computed_value: 0.99502\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [4.07216, -2.97215, -1.00289]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-24.9113, 0.99502, 0.99502]\n\tselected_value (hardened eval): -24.9113\nSample statistics (SOFT training state):\ninput=[-1.94992, 1.26983]\noutput=-2.4601, target=-2.47607\nG (soft training): [0.00373, 0.98723, 0.23695]\n    Step 0:\n        selectors: [1.01957, 1.01815, 0.0, -0.0, 0.0]\n        inputs:    [-5.51336, 4.51835, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -24.9113\n    Step 1:\n        selectors: [-0.61524, -0.70367, 0.29789, -0.0, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.99502\n    Step 2:\n        selectors: [0.02003, 0.06741, 0.06067, 1.24399, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.99502, 0.0]\n        G: 0.2 \u2192 computed_value: 0.99502\nOutput Selector (soft training):\n\tlogits (soft training): [4.07216, -2.97215, -1.00289]\n\tselected_node: 0\n\tintermediate_values (soft training): [-2.47516, -0.4322, -0.33454]\n\tselected_value (soft training): -2.47516\nEarly stopped at step 182\nfinished:\n  - loss_train_capped: 4.531147694195308e-15\n  - loss_train (+reg loss): 0.0002338038757443428\n  - loss_train_criterion: 0.0002338038757443428\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 182 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:50\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/4tcvernd\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234150-4tcvernd/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234150-4tcvernd\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:50\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/4tcvernd\n"
    },
    {
      "operation": "mul",
      "seed": 456,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 198,
      "duration": 22.374794006347656,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "_value: 0.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, 0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -21.07917\n    Step 2:\n        selectors: [1.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, -21.07917, 0.0]\n        G: 1.0 \u2192 computed_value: -4.15863\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.1332, 3.42358, -1.92872]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, -21.07917, -4.15863]\n\tselected_value (hardened eval): -21.07917\nSample statistics (SOFT training state):\ninput=[-0.45757, -1.81058]\noutput=0.82332, target=0.82846\nG (soft training): [0.00395, 0.9696, 0.01085]\n    Step 0:\n        selectors: [0.98303, 1.00912, 0.0, 0.0, 0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.23044, -0.05431, 0.1373, -0.0, -0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -21.07917\n    Step 2:\n        selectors: [0.07572, 0.00656, 0.99201, -0.06118, -0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, -21.07917, 0.0]\n        G: 0.0 \u2192 computed_value: -4.15863\nOutput Selector (soft training):\n\tlogits (soft training): [1.945, -2.17615, 0.82329]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.82762, 0.12517, 0.8449]\n\tselected_value (soft training): 0.82762\nEarly stopped at step 198\nfinished:\n  - loss_train_capped: 4.657254806415476e-15\n  - loss_train (+reg loss): 0.00022171877208165824\n  - loss_train_criterion: 0.00022171877208165824\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 198 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:42:03\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/nrbkgr38\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234203-nrbkgr38/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234203-nrbkgr38\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:42:03\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/nrbkgr38\n"
    },
    {
      "operation": "mul",
      "seed": 789,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 208,
      "duration": 22.790745973587036,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "p 1:\n        selectors: [-1.0, 0.0, 1.0, 0.0, -0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -9.62311\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 0.0, 0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, -9.62311, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [4.03681, -0.42867, -2.74715]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-7.48847, -9.62311, 0.0]\n\tselected_value (hardened eval): -7.48847\nSample statistics (SOFT training state):\ninput=[1.14526, 0.66159]\noutput=0.73416, target=0.7577\nG (soft training): [0.00521, 0.85477, 0.97134]\n    Step 0:\n        selectors: [1.00689, 1.00773, -0.0, 0.0, -0.0]\n        inputs:    [2.13463, -3.50809, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -7.48847\n    Step 1:\n        selectors: [-1.69357, 0.06829, 0.50435, 0.0, -0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -9.62311\n    Step 2:\n        selectors: [-0.28491, -0.19481, -0.02066, 0.15339, 0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, -9.62311, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [4.03681, -0.42867, -2.74715]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.75379, -0.84649, -0.54873]\n\tselected_value (soft training): 0.75379\nEarly stopped at step 208\nfinished:\n  - loss_train_capped: 3.8381883459806705e-15\n  - loss_train (+reg loss): 0.0007111014565452933\n  - loss_train_criterion: 0.0007111014565452933\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 208 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:42:11\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/9vujij7j\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234211-9vujij7j/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234211-9vujij7j\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:42:11\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/9vujij7j\n"
    },
    {
      "operation": "mul",
      "seed": 42,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 111.71699094772339,
      "final_inter_loss": 1.1773474216,
      "final_extra_loss": Infinity,
      "stdout_excerpt": " G: 1.0 \u2192 computed_value: 9.62063\n    Step 1:\n        selectors: [-1.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -9.62063\n    Step 2:\n        selectors: [1.0, -1.0, 1.0, 1.0, 0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, -9.62063, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.15179, -3.77048, 2.93104]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [9.62063, -9.62063, -0.6725]\n\tselected_value (hardened eval): -0.6725\nSample statistics (SOFT training state):\ninput=[-0.57972, 0.83151]\noutput=-0.69888, target=-0.48204\nG (soft training): [0.96207, 0.90166, 0.99961]\n    Step 0:\n        selectors: [0.6473, 1.12744, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\n    Step 1:\n        selectors: [-1.35794, -0.84798, 0.20477, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -9.62063\n    Step 2:\n        selectors: [1.00723, -2.68528, 2.77155, 1.93641, 0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, -9.62063, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (soft training):\n\tlogits (soft training): [-0.15179, -3.77048, 2.93104]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.53276, 0.30189, -0.75655]\n\tselected_value (soft training): -0.75655\nfinished:\n  - loss_train_capped: 1.1280956268310547\n  - loss_train (+reg loss): 0.04237077385187149\n  - loss_train_criterion: 0.04237077385187149\n  - loss_valid_inter: 1.1773474216461182\n  - loss_valid_extra: 112.63907623291016\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:03\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/389dytyr\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234103-389dytyr/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234103-389dytyr\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:03\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/389dytyr\n"
    },
    {
      "operation": "div",
      "seed": 1,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 113.47022891044617,
      "final_inter_loss": 48.0978317261,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [0.0, -0.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [1.28518, 3.05639, -4.00801]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [-5.14657, 1.0, 0.0]\n\tselected_value (hardened eval): 1.0\nSample statistics (SOFT training state):\ninput=[1.37263, -1.28816]\noutput=-1.0598, target=-1.06557\nG (soft training): [0.00093, 0.83229, 0.00511]\n    Step 0:\n        selectors: [0.81124, 0.02538, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [-1.464, 0.38389, 0.5064, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.95783, -0.9677, -1.12308, -0.04037, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [-1.36703, -4.03947, 4.45943]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.2972, -1.04083, -1.06675]\n\tselected_value (soft training): -1.06675\nfinished:\n  - loss_train_capped: 25.252925872802734\n  - loss_train (+reg loss): 14.270607948303223\n  - loss_train_criterion: 14.270607948303223\n  - loss_valid_inter: 48.097835540771484\n  - loss_valid_extra: 1.7093288898468018\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:11\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/k4dvfapw\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234211-k4dvfapw/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234211-k4dvfapw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:11\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/k4dvfapw\n"
    },
    {
      "operation": "div",
      "seed": 7,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 112.75735878944397,
      "final_inter_loss": 21.4715957642,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "0 \u2192 computed_value: 0.86933\n    Step 1:\n        selectors: [-0.0, -1.0, 1.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -4.27724\n    Step 2:\n        selectors: [-0.0, 0.0, 1.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, -4.27724, 0.0]\n        G: 1.0 \u2192 computed_value: 0.86933\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [11.77135, -3.98283, -10.16697]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.86933, -4.27724, 0.86933]\n\tselected_value (hardened eval): 0.86933\nSample statistics (SOFT training state):\ninput=[-0.29971, -0.90332]\noutput=0.34284, target=0.33179\nG (soft training): [0.90859, 0.00073, 0.00862]\n    Step 0:\n        selectors: [-0.41827, 0.28003, -0.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 0.86933\n    Step 1:\n        selectors: [0.8998, 0.05096, -0.04248, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -4.27724\n    Step 2:\n        selectors: [1.92739, -0.95579, -0.10462, -0.96444, -0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, -4.27724, 0.0]\n        G: 0.0 \u2192 computed_value: 0.86933\nOutput Selector (soft training):\n\tlogits (soft training): [-7.07764, 1.75859, 6.60219]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.14512, -0.36513, 0.34842]\n\tselected_value (soft training): 0.34842\nfinished:\n  - loss_train_capped: 3.230555772781372\n  - loss_train (+reg loss): 0.006198884453624487\n  - loss_train_criterion: 0.006198884453624487\n  - loss_valid_inter: 21.47159767150879\n  - loss_valid_extra: 0.5160653591156006\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:26\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/tztwnqv3\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234226-tztwnqv3/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234226-tztwnqv3\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:26\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/tztwnqv3\n"
    },
    {
      "operation": "div",
      "seed": 42,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 114.5568151473999,
      "final_inter_loss": 25.9931411743,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 0.0, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [0.0, 0.0, -0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.16891, -0.39914, 0.56428]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 0.0, 0.0]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[-1.73653, 1.00807]\noutput=-0.07117, target=-1.72263\nG (soft training): [0.43432, 0.51297, 0.61695]\n    Step 0:\n        selectors: [0.14322, 0.00072, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.17104, 0.00192, 0.02621, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [0.13996, 0.00627, -0.03514, -0.14114, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.6 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [-0.16891, -0.39914, 0.56428]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.04192, -0.05145, -0.13301]\n\tselected_value (soft training): -0.13301\nfinished:\n  - loss_train_capped: 17.813949584960938\n  - loss_train (+reg loss): 17.75869369506836\n  - loss_train_criterion: 17.75869369506836\n  - loss_valid_inter: 25.993141174316406\n  - loss_valid_extra: 0.7215508222579956\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:33\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/taaca5mu\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234233-taaca5mu/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234233-taaca5mu\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:33\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/taaca5mu\n"
    },
    {
      "operation": "div",
      "seed": 99,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 113.58342695236206,
      "final_inter_loss": 3020671744.0,
      "final_extra_loss": Infinity,
      "stdout_excerpt": " [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, 0.0, -1.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 6123.234\n    Step 2:\n        selectors: [0.0, -1.0, 1.0, 1.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 6123.234, 0.0]\n        G: 1.0 \u2192 computed_value: 6118.08743\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.15365, -0.75076, -0.3695]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.0, 6123.234, 6118.08743]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[-1.23481, 1.20193]\noutput=-0.37765, target=-1.02736\nG (soft training): [0.66677, 0.20959, 0.71672]\n    Step 0:\n        selectors: [0.09316, 0.00458, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.7 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.31663, 2e-05, -0.58303, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 6123.234\n    Step 2:\n        selectors: [0.21604, -0.50841, 0.98374, 0.62391, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 6123.234, 0.0]\n        G: 0.7 \u2192 computed_value: 6118.08743\nOutput Selector (soft training):\n\tlogits (soft training): [0.15365, -0.75076, -0.3695]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.12379, -0.3701, -0.81114]\n\tselected_value (soft training): -0.12379\nfinished:\n  - loss_train_capped: 67135744.0\n  - loss_train (+reg loss): 26.05424690246582\n  - loss_train_criterion: 26.05424690246582\n  - loss_valid_inter: 3020672000.0\n  - loss_valid_extra: 1.4552334547042847\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:55\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/t1ownejo\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234255-t1ownejo/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234255-t1ownejo\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:55\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/t1ownejo\n"
    },
    {
      "operation": "div",
      "seed": 221,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 888,
      "duration": 57.510185956954956,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "3, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, -1.0, 0.0, -1.0, 0.0]\n        inputs:    [4.187, 3.72613, 1.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.12368\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.15809, -2.89036, 4.01682]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.0, 1.12368]\n\tselected_value (hardened eval): 1.12368\nSample statistics (SOFT training state):\ninput=[-1.44072, 1.45898]\noutput=-0.76723, target=-0.98748\nG (soft training): [0.90698, 0.04376, 0.86417]\n    Step 0:\n        selectors: [0.01032, -1.70173, -0.0, 0.0, 0.0]\n        inputs:    [4.187, 3.72613, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.98257, -1.08267, -0.1022, 0.0, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.05451, 0.26015, 0.96024, -0.07985, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 1.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.12368\nOutput Selector (soft training):\n\tlogits (soft training): [-0.866, 2.67516, -2.26606]\n\tselected_node: 1\n\tintermediate_values (soft training): [-1.72372, -0.7392, -0.81034]\n\tselected_value (soft training): -0.7392\nEarly stopped at step 888\nfinished:\n  - loss_train_capped: 3.075113135051627e-13\n  - loss_train (+reg loss): 0.12655314803123474\n  - loss_train_criterion: 0.12655314803123474\n  - loss_valid_inter: 1.5699225773546654e-13\n  - loss_valid_extra: 3.895639202661141e-15\n\nModel (/checkpoint)  trained for 888 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:18\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/levu4yij\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234419-levu4yij/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234419-levu4yij\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:18\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/levu4yij\n"
    },
    {
      "operation": "div",
      "seed": 234,
      "success": false,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 74.95577907562256,
      "final_inter_loss": 48.8258972168,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "2174, target=0.96873\nG (hardened eval): [1.0, 0.0, 1.0]\n    Step 0:\n        selectors: [-1.0, -0.0, -0.0, -0.0, -0.0]\n        inputs:    [4.62836, 4.77774, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -4.62836\n    Step 1:\n        selectors: [1.0, 0.0, 1.0, -0.0, 0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -21.42174\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, 0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, -21.42174, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.85078, 4.14553, -2.58284]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [-4.62836, -21.42174, 0.0]\n\tselected_value (hardened eval): -21.42174\nSample statistics (SOFT training state):\ninput=[1.87648, -1.42256]\noutput=-1.35698, target=-1.31909\nG (soft training): [0.02574, 0.98298, 0.02045]\n    Step 0:\n        selectors: [0.56267, 0.0282, 0.0, 0.0, 0.0]\n        inputs:    [4.62836, 4.77774, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -4.62836\n    Step 1:\n        selectors: [-0.34328, -0.44966, -0.33638, 0.0, -0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -21.42174\n    Step 2:\n        selectors: [2.94751, -0.92475, -3.32521, -0.04327, -0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, -21.42174, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.07743, -4.88006, 4.35868]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.43018, -0.47835, -1.3956]\n\tselected_value (soft training): -1.3956\n> /Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py(1206)<module>()\n-> raise\n(Pdb) \n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:28\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/69hzbe7s\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234428-69hzbe7s/logs\u001b[0m\n",
      "stderr_excerpt": ", **kwargs)\n  File \"/opt/homebrew/anaconda3/envs/nalm/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py\", line 104, in _get_total_norm\n    raise RuntimeError(\nRuntimeError: The total norm of order 2.0 for gradients from `parameters` is non-finite, so it cannot be clipped. To disable this error and scale the gradients by the non-finite norm anyway, set `error_if_nonfinite=False`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 1206, in <module>\n    raise\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 1206, in <module>\n    raise\n  File \"/opt/homebrew/anaconda3/envs/nalm/lib/python3.10/bdb.py\", line 90, in trace_dispatch\n    return self.dispatch_line(frame)\n  File \"/opt/homebrew/anaconda3/envs/nalm/lib/python3.10/bdb.py\", line 115, in dispatch_line\n    if self.quitting: raise BdbQuit\nbdb.BdbQuit\n"
    },
    {
      "operation": "div",
      "seed": 345,
      "success": true,
      "grokked": true,
      "early_stopped": true,
      "grok_step": 1063,
      "duration": 67.54205107688904,
      "final_inter_loss": 0.0,
      "final_extra_loss": 0.0,
      "stdout_excerpt": "ectors: [-0.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.73534\n    Step 2:\n        selectors: [-1.0, 1.0, -0.0, -0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, -5.73534, 0.0]\n        G: 1.0 \u2192 computed_value: -0.31501\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.91466, -4.8017, -0.83122]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.94508, -5.73534, -0.31501]\n\tselected_value (hardened eval): 0.94508\nSample statistics (SOFT training state):\ninput=[1.60332, -0.31912]\noutput=-5.00385, target=-5.02417\nG (soft training): [0.01175, 0.86361, 0.52895]\n    Step 0:\n        selectors: [1.0107, -1.0418, -0.0, 0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.94508\n    Step 1:\n        selectors: [-0.48127, 1.03278, -0.16626, 0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -5.73534\n    Step 2:\n        selectors: [-0.98898, 0.96823, -0.09388, -0.02884, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, -5.73534, 0.0]\n        G: 0.5 \u2192 computed_value: -0.31501\nOutput Selector (soft training):\n\tlogits (soft training): [2.91466, -4.8017, -0.83122]\n\tselected_node: 0\n\tintermediate_values (soft training): [-5.10275, -0.46206, -0.9013]\n\tselected_value (soft training): -5.10275\nEarly stopped at step 1063\nfinished:\n  - loss_train_capped: 5.266337902874382e-14\n  - loss_train (+reg loss): 0.12980201840400696\n  - loss_train_criterion: 0.12980201840400696\n  - loss_valid_inter: 1.5699225773546654e-13\n  - loss_valid_extra: 3.895639202661141e-15\n\nModel (/checkpoint)  trained for 1063 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:48\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/w6ydh9nw\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234449-w6ydh9nw/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234449-w6ydh9nw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:48\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/w6ydh9nw\n"
    },
    {
      "operation": "div",
      "seed": 123,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 114.4033191204071,
      "final_inter_loss": 48.0978317261,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [1.0, -0.0, -0.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 4.47407\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.37881, 0.09733, -0.37385]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [1.0, 0.0, 4.47407]\n\tselected_value (hardened eval): 1.0\nSample statistics (SOFT training state):\ninput=[-0.70395, -0.97306]\noutput=0.7034, target=0.72344\nG (soft training): [0.00475, 0.00361, 0.49003]\n    Step 0:\n        selectors: [0.73604, 0.00055, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.92359, -1.00867, -1.21962, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [-0.88343, 1.26615, 0.02479, 0.05287, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 4.47407\nOutput Selector (soft training):\n\tlogits (soft training): [-3.46519, 3.37465, -1.79114]\n\tselected_node: 1\n\tintermediate_values (soft training): [-0.5215, 0.70887, -0.02654]\n\tselected_value (soft training): 0.70887\nfinished:\n  - loss_train_capped: 42.97352981567383\n  - loss_train (+reg loss): 26.073104858398438\n  - loss_train_criterion: 26.073104858398438\n  - loss_valid_inter: 48.097835540771484\n  - loss_valid_extra: 1.7093288898468018\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:05\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kra1wec\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234405-5kra1wec/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234405-5kra1wec\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:05\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kra1wec\n"
    },
    {
      "operation": "div",
      "seed": 456,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 109.55329322814941,
      "final_inter_loss": 25.9931411743,
      "final_extra_loss": Infinity,
      "stdout_excerpt": "0, 0.0, -0.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [1.0, 0.0, 1.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [-0.0, 0.0, 1.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.53748, 7.42579, -5.13841]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, 0.0, 0.0]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[1.12516, 0.56056]\noutput=2.03133, target=2.00722\nG (soft training): [2e-05, 0.31791, 0.00465]\n    Step 0:\n        selectors: [2.9221, -0.07694, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.50495, -1.2756, -0.08955, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.3 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [2.3504, -1.06348, -0.46812, 0.0021, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.31267, -6.99672, 5.8817]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.47573, 0.247, 2.03345]\n\tselected_value (soft training): 2.03345\nfinished:\n  - loss_train_capped: 41.38689041137695\n  - loss_train (+reg loss): 35.64674758911133\n  - loss_train_criterion: 35.64674758911133\n  - loss_valid_inter: 25.993141174316406\n  - loss_valid_extra: 0.7215508222579956\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:45:16\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ch9fjjkt\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234516-ch9fjjkt/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234516-ch9fjjkt\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:45:16\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ch9fjjkt\nwandb: WARNING Tried to log to step 2000 that is less than the current step 2001. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
    },
    {
      "operation": "div",
      "seed": 789,
      "success": true,
      "grokked": false,
      "early_stopped": false,
      "grok_step": null,
      "duration": 104.72264194488525,
      "final_inter_loss": 21.4715957642,
      "final_extra_loss": Infinity,
      "stdout_excerpt": " inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.86933\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.28196, -2.39413, 4.0243]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, 0.0, 0.86933]\n\tselected_value (hardened eval): 0.86933\nSample statistics (SOFT training state):\ninput=[1.01046, 1.60341]\noutput=0.5902, target=0.63019\nG (soft training): [0.80812, 0.84831, 0.01364]\n    Step 0:\n        selectors: [-0.23597, -0.21122, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.30398, 0.21684, 0.37334, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [1.05127, -1.02366, -0.08964, 0.0144, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.86933\nOutput Selector (soft training):\n\tlogits (soft training): [-0.28196, -2.39413, 4.0243]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.31475, -0.16243, 0.60363]\n\tselected_value (soft training): 0.60363\nfinished:\n  - loss_train_capped: 3.8393702507019043\n  - loss_train (+reg loss): 0.005665719974786043\n  - loss_train_criterion: 0.005665719974786043\n  - loss_valid_inter: 21.47159767150879\n  - loss_valid_extra: 0.5160653591156006\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:45:43\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v6hd07th\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234543-v6hd07th/logs\u001b[0m\n",
      "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234543-v6hd07th\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:45:43\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v6hd07th\n"
    }
  ],
  "analysis": {
    "total_experiments": 40,
    "successful_runs": 39,
    "operations": {
      "add": {
        "total_seeds": 10,
        "successful_runs": 10,
        "grokked_runs": 9,
        "success_rate": 1.0,
        "grok_rate": 0.9,
        "grok_rate_given_success": 0.9,
        "avg_grok_step": 184.66666666666666,
        "median_grok_step": 178.0,
        "best_seed_result": {
          "operation": "add",
          "seed": 7,
          "success": true,
          "grokked": true,
          "early_stopped": true,
          "grok_step": 170,
          "duration": 22.236414909362793,
          "final_inter_loss": 0.0,
          "final_extra_loss": 0.0,
          "stdout_excerpt": "\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -2.50159\n    Step 2:\n        selectors: [-0.0, -0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, -2.50159, 0.0]\n        G: 0.0 \u2192 computed_value: -2.50159\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.92057, 3.59711, -2.16112]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [3.37315, -2.50159, -2.50159]\n\tselected_value (hardened eval): -2.50159\nSample statistics (SOFT training state):\ninput=[1.93651, 1.8975]\noutput=3.89285, target=3.83401\nG (soft training): [0.97976, 0.0433, 0.99683]\n    Step 0:\n        selectors: [1.61924, 0.14347, -0.0, -0.0, -0.0]\n        inputs:    [-5.87474, 3.37315, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.37315\n    Step 1:\n        selectors: [-0.01454, -0.35208, 0.46332, 0.0, -0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -2.50159\n    Step 2:\n        selectors: [1.44328, 1.32174, -0.39177, 0.00175, -0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, -2.50159, 0.0]\n        G: 1.0 \u2192 computed_value: -2.50159\nOutput Selector (soft training):\n\tlogits (soft training): [1.69164, -2.8559, 2.90034]\n\tselected_node: 2\n\tintermediate_values (soft training): [3.87851, 1.47953, 3.90477]\n\tselected_value (soft training): 3.90477\nEarly stopped at step 170\nfinished:\n  - loss_train_capped: 4.6247740575035886e-15\n  - loss_train (+reg loss): 0.0003791885683313012\n  - loss_train_criterion: 0.0003791885683313012\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 170 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ae81h3zf\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-ae81h3zf/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-ae81h3zf\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ae81h3zf\n"
        },
        "worst_successful_result": {
          "operation": "add",
          "seed": 42,
          "success": true,
          "grokked": false,
          "early_stopped": false,
          "grok_step": null,
          "duration": 113.33402705192566,
          "final_inter_loss": 0.6720571518,
          "final_extra_loss": Infinity,
          "stdout_excerpt": "47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 5.14657\n    Step 2:\n        selectors: [1.0, 1.0, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 5.14657, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-4.08727, 3.10393, 2.56144]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 5.14657, 9.62063]\n\tselected_value (hardened eval): 5.14657\nSample statistics (SOFT training state):\ninput=[-0.57972, 0.83151]\noutput=0.25323, target=0.25179\nG (soft training): [0.00405, 0.99952, 0.99937]\n    Step 0:\n        selectors: [0.09854, 0.02211, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.4943, 1.40287, -0.02138, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 5.14657\n    Step 2:\n        selectors: [2.04458, 0.84977, 0.02434, -0.39078, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 5.14657, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\nOutput Selector (soft training):\n\tlogits (soft training): [-4.08727, 3.10393, 2.56144]\n\tselected_node: 1\n\tintermediate_values (soft training): [0.8882, 0.86059, -0.79244]\n\tselected_value (soft training): 0.86059\nfinished:\n  - loss_train_capped: 0.6496227383613586\n  - loss_train (+reg loss): 4.824825009563938e-05\n  - loss_train_criterion: 4.824825009563938e-05\n  - loss_valid_inter: 0.6720571517944336\n  - loss_valid_extra: 8.708630561828613\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/clj25y4w\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-clj25y4w/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-clj25y4w\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/clj25y4w\n"
        },
        "grokked_seeds": [
          7,
          99,
          1,
          123,
          221,
          234,
          456,
          345,
          789
        ],
        "failed_seeds": [],
        "non_grok_seeds": [
          42
        ],
        "all_results": [
          {
            "operation": "add",
            "seed": 7,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 170,
            "duration": 22.236414909362793,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -2.50159\n    Step 2:\n        selectors: [-0.0, -0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, -2.50159, 0.0]\n        G: 0.0 \u2192 computed_value: -2.50159\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.92057, 3.59711, -2.16112]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [3.37315, -2.50159, -2.50159]\n\tselected_value (hardened eval): -2.50159\nSample statistics (SOFT training state):\ninput=[1.93651, 1.8975]\noutput=3.89285, target=3.83401\nG (soft training): [0.97976, 0.0433, 0.99683]\n    Step 0:\n        selectors: [1.61924, 0.14347, -0.0, -0.0, -0.0]\n        inputs:    [-5.87474, 3.37315, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.37315\n    Step 1:\n        selectors: [-0.01454, -0.35208, 0.46332, 0.0, -0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -2.50159\n    Step 2:\n        selectors: [1.44328, 1.32174, -0.39177, 0.00175, -0.0]\n        inputs:    [-5.87474, 3.37315, 3.37315, -2.50159, 0.0]\n        G: 1.0 \u2192 computed_value: -2.50159\nOutput Selector (soft training):\n\tlogits (soft training): [1.69164, -2.8559, 2.90034]\n\tselected_node: 2\n\tintermediate_values (soft training): [3.87851, 1.47953, 3.90477]\n\tselected_value (soft training): 3.90477\nEarly stopped at step 170\nfinished:\n  - loss_train_capped: 4.6247740575035886e-15\n  - loss_train (+reg loss): 0.0003791885683313012\n  - loss_train_criterion: 0.0003791885683313012\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 170 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ae81h3zf\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-ae81h3zf/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-ae81h3zf\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ae81h3zf\n"
          },
          {
            "operation": "add",
            "seed": 99,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 178,
            "duration": 22.097729921340942,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "e: -6.40207\n    Step 1:\n        selectors: [0.0, -0.0, -0.0, 0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 1.0, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.58899, -0.72341, -2.46922]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-6.40207, 1.0, 1.0]\n\tselected_value (hardened eval): -6.40207\nSample statistics (SOFT training state):\ninput=[1.92771, 0.21161]\noutput=2.13512, target=2.13932\nG (soft training): [0.99627, 0.20222, 0.03311]\n    Step 0:\n        selectors: [1.009, 1.03938, -0.0, -0.0, 0.0]\n        inputs:    [-2.39794, -4.00413, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -6.40207\n    Step 1:\n        selectors: [0.43925, -0.11377, -0.25428, 0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.06407, -0.20769, -0.00696, 0.94065, -0.0]\n        inputs:    [-2.39794, -4.00413, -6.40207, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [2.58899, -0.72341, -2.46922]\n\tselected_node: 0\n\tintermediate_values (soft training): [2.17748, 1.09654, 1.42342]\n\tselected_value (soft training): 2.17748\nEarly stopped at step 178\nfinished:\n  - loss_train_capped: 4.6160822289153386e-15\n  - loss_train (+reg loss): 0.0005652736872434616\n  - loss_train_criterion: 0.0005652736872434616\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 178 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g91bzoti\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-g91bzoti/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-g91bzoti\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g91bzoti\n"
          },
          {
            "operation": "add",
            "seed": 1,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 183,
            "duration": 23.468570232391357,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "126\n    Step 1:\n        selectors: [0.0, -0.0, 0.0, 0.0, -0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.82863\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.57296, -3.02173, 3.7646]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-2.27126, 1.0, -5.82863]\n\tselected_value (hardened eval): -5.82863\nSample statistics (SOFT training state):\ninput=[-0.88516, 1.72836]\noutput=0.83183, target=0.84319\nG (soft training): [0.43487, 0.04549, 0.99304]\n    Step 0:\n        selectors: [0.28801, 1.30187, -0.0, 0.0, -0.0]\n        inputs:    [-3.55737, -2.27126, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: -2.27126\n    Step 1:\n        selectors: [0.15913, -0.02203, 0.08013, 0.0, -0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.01048, 1.07934, -0.0633, -0.02881, 0.0]\n        inputs:    [-3.55737, -2.27126, -2.27126, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.82863\nOutput Selector (soft training):\n\tlogits (soft training): [-0.57296, -3.02173, 3.7646]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.97334, 0.86598, 0.81688]\n\tselected_value (soft training): 0.81688\nEarly stopped at step 183\nfinished:\n  - loss_train_capped: 1.113855101902983e-14\n  - loss_train (+reg loss): 0.0002933297655545175\n  - loss_train_criterion: 0.0002933297655545175\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 183 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/aampgqsw\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-aampgqsw/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-aampgqsw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/aampgqsw\n"
          },
          {
            "operation": "add",
            "seed": 123,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 152,
            "duration": 20.882658004760742,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "1.0\n    Step 1:\n        selectors: [0.0, 1.0, 0.0, 0.0, 0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -2.74181\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, -0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, -2.74181, 0.0]\n        G: 1.0 \u2192 computed_value: -7.32969\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.54113, 0.41014, 3.18632]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, -2.74181, -7.32969]\n\tselected_value (hardened eval): -7.32969\nSample statistics (SOFT training state):\ninput=[0.90836, -0.05174]\noutput=0.84462, target=0.85663\nG (soft training): [0.99675, 0.68358, 0.03752]\n    Step 0:\n        selectors: [1.00911, 1.03136, 0.0, 0.0, -0.0]\n        inputs:    [-4.58788, -2.74181, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.57229, -0.12865, -0.37202, -0.0, -0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, 0.0, 0.0]\n        G: 0.7 \u2192 computed_value: -2.74181\n    Step 2:\n        selectors: [-0.10955, -0.18181, -0.04278, 1.15736, 0.0]\n        inputs:    [-4.58788, -2.74181, 1.0, -2.74181, 0.0]\n        G: 0.0 \u2192 computed_value: -7.32969\nOutput Selector (soft training):\n\tlogits (soft training): [2.76275, -1.39271, -2.51112]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.84952, 0.57865, 0.70153]\n\tselected_value (soft training): 0.84952\nEarly stopped at step 152\nfinished:\n  - loss_train_capped: 3.903036764905986e-15\n  - loss_train (+reg loss): 0.00040152965812012553\n  - loss_train_criterion: 0.00040152965812012553\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 152 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:39\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/cp21pnfx\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233839-cp21pnfx/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233839-cp21pnfx\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:39\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/cp21pnfx\n"
          },
          {
            "operation": "add",
            "seed": 221,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 178,
            "duration": 21.878474235534668,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "e: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, -0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.60618\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 1.60618, 0.0]\n        G: 1.0 \u2192 computed_value: -6.40207\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-3.47604, 0.17633, 3.18269]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.60618, -6.40207]\n\tselected_value (hardened eval): -6.40207\nSample statistics (SOFT training state):\ninput=[-0.66813, -1.94609]\noutput=-2.61897, target=-2.61421\nG (soft training): [0.02552, 0.96754, 0.99657]\n    Step 0:\n        selectors: [0.03036, -0.06161, 0.0, -0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.27176, -0.62854, 0.16474, -0.0, -0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.60618\n    Step 2:\n        selectors: [1.12249, 1.02943, -0.01896, -0.093, 0.0]\n        inputs:    [-2.39794, -4.00413, 1.0, 1.60618, 0.0]\n        G: 1.0 \u2192 computed_value: -6.40207\nOutput Selector (soft training):\n\tlogits (soft training): [-3.47604, 0.17633, 3.18269]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.84389, 0.50117, -2.77777]\n\tselected_value (soft training): -2.77777\nEarly stopped at step 178\nfinished:\n  - loss_train_capped: 4.895381178877077e-15\n  - loss_train (+reg loss): 0.0008148237247951329\n  - loss_train_criterion: 0.0008148237247951329\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 178 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:39\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/54q0t3jc\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233839-54q0t3jc/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233839-54q0t3jc\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:39\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/54q0t3jc\n"
          },
          {
            "operation": "add",
            "seed": 234,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 246,
            "duration": 24.207001209259033,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": ".0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.76581\n    Step 2:\n        selectors: [1.0, -0.0, 1.0, 0.0, 0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 1.76581, 0.0]\n        G: 1.0 \u2192 computed_value: -2.51491\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.12249, 3.3987, -1.65079]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 1.76581, -2.51491]\n\tselected_value (hardened eval): 1.76581\nSample statistics (SOFT training state):\ninput=[0.89154, 0.62703]\noutput=1.5372, target=1.51857\nG (soft training): [0.99928, 0.01824, 0.00192]\n    Step 0:\n        selectors: [1.76174, 1.00974, 0.0, 0.0, 0.0]\n        inputs:    [-3.51491, 5.28072, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.08075, -0.4602, 0.64955, 0.0, -0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.76581\n    Step 2:\n        selectors: [-0.0055, 1.02214, -7e-05, -0.00125, -0.0]\n        inputs:    [-3.51491, 5.28072, 1.0, 1.76581, 0.0]\n        G: 0.0 \u2192 computed_value: -2.51491\nOutput Selector (soft training):\n\tlogits (soft training): [2.35447, -3.6095, 2.04058]\n\tselected_node: 0\n\tintermediate_values (soft training): [2.20663, 2.0801, 0.61903]\n\tselected_value (soft training): 2.20663\nEarly stopped at step 246\nfinished:\n  - loss_train_capped: 4.786864823389013e-15\n  - loss_train (+reg loss): 0.00046885700430721045\n  - loss_train_criterion: 0.00046885700430721045\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 246 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:40\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v1dgevje\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233840-v1dgevje/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233840-v1dgevje\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:40\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v1dgevje\n"
          },
          {
            "operation": "add",
            "seed": 456,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 168,
            "duration": 21.401931047439575,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "  Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, 0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.37101\n    Step 2:\n        selectors: [-0.0, -0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.37101, 0.0]\n        G: 0.0 \u2192 computed_value: 0.37101\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.87509, 4.40768, -2.55521]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [5.55641, 0.37101, 0.37101]\n\tselected_value (hardened eval): 0.37101\nSample statistics (SOFT training state):\ninput=[1.51957, 0.40007]\noutput=1.89094, target=1.91964\nG (soft training): [0.43375, 0.03312, 0.99601]\n    Step 0:\n        selectors: [1.42894, -0.01468, 0.0, 0.0, 0.0]\n        inputs:    [-5.18539, 5.55641, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: 5.55641\n    Step 1:\n        selectors: [-0.10523, -0.1206, 1.01706, -0.0, -0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.37101\n    Step 2:\n        selectors: [1.06654, 1.03246, -0.09073, 0.01997, -0.0]\n        inputs:    [-5.18539, 5.55641, 5.55641, 0.37101, 0.0]\n        G: 1.0 \u2192 computed_value: 0.37101\nOutput Selector (soft training):\n\tlogits (soft training): [-0.16801, -2.49441, 3.22347]\n\tselected_node: 2\n\tintermediate_values (soft training): [2.60532, 2.90244, 1.86357]\n\tselected_value (soft training): 1.86357\nEarly stopped at step 168\nfinished:\n  - loss_train_capped: 4.544341502898215e-15\n  - loss_train (+reg loss): 0.0005102852592244744\n  - loss_train_criterion: 0.0005102852592244744\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 168 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:39:00\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/8pkbewq2\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233900-8pkbewq2/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233900-8pkbewq2\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:39:00\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/8pkbewq2\n"
          },
          {
            "operation": "add",
            "seed": 345,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 196,
            "duration": 22.906141996383667,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": ":\n        selectors: [1.0, 0.0, 0.0, 0.0, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.62527\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 3.62527, 0.0]\n        G: 1.0 \u2192 computed_value: 0.40106\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.52722, -0.78557, 2.18211]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-3.22421, 3.62527, 0.40106]\n\tselected_value (hardened eval): 0.40106\nSample statistics (SOFT training state):\ninput=[0.97654, -0.26238]\noutput=0.71944, target=0.71416\nG (soft training): [0.03862, 0.94282, 0.98832]\n    Step 0:\n        selectors: [0.17658, 0.54487, -0.0, 0.0, -0.0]\n        inputs:    [3.62527, -3.22421, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -3.22421\n    Step 1:\n        selectors: [1.355, 0.13753, 0.19357, 0.0, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 3.62527\n    Step 2:\n        selectors: [1.34589, 1.12329, -0.08998, -0.23765, -0.0]\n        inputs:    [3.62527, -3.22421, -3.22421, 3.62527, 0.0]\n        G: 1.0 \u2192 computed_value: 0.40106\nOutput Selector (soft training):\n\tlogits (soft training): [-2.52722, -0.78557, 2.18211]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.08733, 1.27404, 0.69819]\n\tselected_value (soft training): 0.69819\nEarly stopped at step 196\nfinished:\n  - loss_train_capped: 4.9061512028014155e-15\n  - loss_train (+reg loss): 0.0003634786990005523\n  - loss_train_criterion: 0.0003634786990005523\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 196 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:39:00\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u4xfu32k\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233900-u4xfu32k/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233900-u4xfu32k\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:39:00\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u4xfu32k\n"
          },
          {
            "operation": "add",
            "seed": 789,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 191,
            "duration": 21.57179617881775,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "   G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, 0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: -10.31961\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-4.2421, -2.18177, 3.72415]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.0, -10.31961]\n\tselected_value (hardened eval): -10.31961\nSample statistics (SOFT training state):\ninput=[1.77139, -1.29738]\noutput=0.49226, target=0.47401\nG (soft training): [0.9975, 0.97876, 0.04194]\n    Step 0:\n        selectors: [0.99198, 1.04408, -0.0, 0.0, -0.0]\n        inputs:    [-5.69853, -4.62108, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.44866, -0.31764, 0.01559, 0.0, -0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [0.0268, -0.35241, 0.35296, 0.39135, 0.0]\n        inputs:    [-5.69853, -4.62108, 1.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: -10.31961\nOutput Selector (soft training):\n\tlogits (soft training): [4.16015, 0.71575, -2.87142]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.40443, 3.24294, 0.51683]\n\tselected_value (soft training): 0.40443\nEarly stopped at step 191\nfinished:\n  - loss_train_capped: 4.423142099574439e-15\n  - loss_train (+reg loss): 0.0005646094214171171\n  - loss_train_criterion: 0.0005646094214171171\n  - loss_valid_inter: 4.511680759485036e-15\n  - loss_valid_extra: 1.1870144895221452e-13\n\nModel (/checkpoint)  trained for 191 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:39:04\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/q8m7sk42\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233904-q8m7sk42/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233904-q8m7sk42\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:39:04\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/q8m7sk42\n"
          },
          {
            "operation": "add",
            "seed": 42,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 113.33402705192566,
            "final_inter_loss": 0.6720571518,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 5.14657\n    Step 2:\n        selectors: [1.0, 1.0, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 5.14657, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-4.08727, 3.10393, 2.56144]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 5.14657, 9.62063]\n\tselected_value (hardened eval): 5.14657\nSample statistics (SOFT training state):\ninput=[-0.57972, 0.83151]\noutput=0.25323, target=0.25179\nG (soft training): [0.00405, 0.99952, 0.99937]\n    Step 0:\n        selectors: [0.09854, 0.02211, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.4943, 1.40287, -0.02138, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 5.14657\n    Step 2:\n        selectors: [2.04458, 0.84977, 0.02434, -0.39078, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 5.14657, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\nOutput Selector (soft training):\n\tlogits (soft training): [-4.08727, 3.10393, 2.56144]\n\tselected_node: 1\n\tintermediate_values (soft training): [0.8882, 0.86059, -0.79244]\n\tselected_value (soft training): 0.86059\nfinished:\n  - loss_train_capped: 0.6496227383613586\n  - loss_train (+reg loss): 4.824825009563938e-05\n  - loss_train_criterion: 4.824825009563938e-05\n  - loss_valid_inter: 0.6720571517944336\n  - loss_valid_extra: 8.708630561828613\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - add - 2025-08-22 23:38:17\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/clj25y4w\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233817-clj25y4w/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233817-clj25y4w\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - add - 2025-08-22 23:38:17\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/clj25y4w\n"
          }
        ]
      },
      "sub": {
        "total_seeds": 10,
        "successful_runs": 10,
        "grokked_runs": 9,
        "success_rate": 1.0,
        "grok_rate": 0.9,
        "grok_rate_given_success": 0.9,
        "avg_grok_step": 349.77777777777777,
        "median_grok_step": 334.0,
        "best_seed_result": {
          "operation": "sub",
          "seed": 1,
          "success": true,
          "grokked": true,
          "early_stopped": true,
          "grok_step": 282,
          "duration": 26.8727810382843,
          "final_inter_loss": 0.0,
          "final_extra_loss": 0.0,
          "stdout_excerpt": " \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [2.3799, 3.27348, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.89358\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 1.0, 0.0]\n        inputs:    [2.3799, 3.27348, 1.0, -0.89358, 0.0]\n        G: 1.0 \u2192 computed_value: -1.78716\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.33842, 2.30904, -0.37735]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, -0.89358, -1.78716]\n\tselected_value (hardened eval): -0.89358\nSample statistics (SOFT training state):\ninput=[1.04785, 0.11851]\noutput=1.0006, target=0.92934\nG (soft training): [0.97773, 0.25195, 0.02982]\n    Step 0:\n        selectors: [0.96917, -0.97634, 0.0, -0.0, 0.0]\n        inputs:    [2.3799, 3.27348, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.08226, -0.04288, 0.30162, -0.0, 0.0]\n        inputs:    [2.3799, 3.27348, 1.0, 0.0, 0.0]\n        G: 0.3 \u2192 computed_value: -0.89358\n    Step 2:\n        selectors: [-0.06809, 0.00015, 1.08751, -0.57778, -0.0]\n        inputs:    [2.3799, 3.27348, 1.0, -0.89358, 0.0]\n        G: 0.0 \u2192 computed_value: -1.78716\nOutput Selector (soft training):\n\tlogits (soft training): [0.62972, -2.40162, 1.1386]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.96283, 0.85966, 1.02739]\n\tselected_value (soft training): 1.02739\nEarly stopped at step 282\nfinished:\n  - loss_train_capped: 4.746454999057879e-15\n  - loss_train (+reg loss): 0.0009271769085898995\n  - loss_train_criterion: 0.0009271769085898995\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 282 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:22\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/n8hbv5vh\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233922-n8hbv5vh/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233922-n8hbv5vh\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:22\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/n8hbv5vh\n"
        },
        "worst_successful_result": {
          "operation": "sub",
          "seed": 234,
          "success": true,
          "grokked": false,
          "early_stopped": false,
          "grok_step": null,
          "duration": 113.40524077415466,
          "final_inter_loss": 0.6413063407,
          "final_extra_loss": Infinity,
          "stdout_excerpt": "omputed_value: -5.14657\n    Step 1:\n        selectors: [0.0, -1.0, 1.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -10.29313\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, -10.29313, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.72302, -1.17329, 1.1792]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-5.14657, -10.29313, -0.6725]\n\tselected_value (hardened eval): -0.6725\nSample statistics (SOFT training state):\ninput=[-1.41493, -1.46763]\noutput=0.00409, target=0.05271\nG (soft training): [0.96114, 0.0003, 0.00459]\n    Step 0:\n        selectors: [0.0, -2e-05, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [-0.00225, 0.00379, -1.03878, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -10.29313\n    Step 2:\n        selectors: [1.02565, 0.05174, -0.00154, -0.02281, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, -10.29313, 0.0]\n        G: 0.0 \u2192 computed_value: -0.6725\nOutput Selector (soft training):\n\tlogits (soft training): [-2.55567, 0.27934, 2.05017]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.00812, 7.53012, -1.27681]\n\tselected_value (soft training): -1.27681\nfinished:\n  - loss_train_capped: 0.6895342469215393\n  - loss_train (+reg loss): 0.0016467851819470525\n  - loss_train_criterion: 0.0016467851819470525\n  - loss_valid_inter: 0.6413062810897827\n  - loss_valid_extra: 8.613598823547363\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:10\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rzsfq5ak\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234010-rzsfq5ak/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234010-rzsfq5ak\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:10\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rzsfq5ak\n"
        },
        "grokked_seeds": [
          1,
          7,
          42,
          99,
          123,
          221,
          456,
          345,
          789
        ],
        "failed_seeds": [],
        "non_grok_seeds": [
          234
        ],
        "all_results": [
          {
            "operation": "sub",
            "seed": 1,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 282,
            "duration": 26.8727810382843,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": " \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [2.3799, 3.27348, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.89358\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 1.0, 0.0]\n        inputs:    [2.3799, 3.27348, 1.0, -0.89358, 0.0]\n        G: 1.0 \u2192 computed_value: -1.78716\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.33842, 2.30904, -0.37735]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, -0.89358, -1.78716]\n\tselected_value (hardened eval): -0.89358\nSample statistics (SOFT training state):\ninput=[1.04785, 0.11851]\noutput=1.0006, target=0.92934\nG (soft training): [0.97773, 0.25195, 0.02982]\n    Step 0:\n        selectors: [0.96917, -0.97634, 0.0, -0.0, 0.0]\n        inputs:    [2.3799, 3.27348, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.08226, -0.04288, 0.30162, -0.0, 0.0]\n        inputs:    [2.3799, 3.27348, 1.0, 0.0, 0.0]\n        G: 0.3 \u2192 computed_value: -0.89358\n    Step 2:\n        selectors: [-0.06809, 0.00015, 1.08751, -0.57778, -0.0]\n        inputs:    [2.3799, 3.27348, 1.0, -0.89358, 0.0]\n        G: 0.0 \u2192 computed_value: -1.78716\nOutput Selector (soft training):\n\tlogits (soft training): [0.62972, -2.40162, 1.1386]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.96283, 0.85966, 1.02739]\n\tselected_value (soft training): 1.02739\nEarly stopped at step 282\nfinished:\n  - loss_train_capped: 4.746454999057879e-15\n  - loss_train (+reg loss): 0.0009271769085898995\n  - loss_train_criterion: 0.0009271769085898995\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 282 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:22\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/n8hbv5vh\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233922-n8hbv5vh/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233922-n8hbv5vh\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:22\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/n8hbv5vh\n"
          },
          {
            "operation": "sub",
            "seed": 7,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 316,
            "duration": 28.284199953079224,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, 0.0, 0.0, 0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.62717\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 1.0, 0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, -8.62717, 0.0]\n        G: 1.0 \u2192 computed_value: -17.25434\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.87341, 2.72882, -1.03841]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, -8.62717, -17.25434]\n\tselected_value (hardened eval): -8.62717\nSample statistics (SOFT training state):\ninput=[1.76364, -1.92693]\noutput=3.90232, target=3.69057\nG (soft training): [0.97547, 0.15669, 0.01305]\n    Step 0:\n        selectors: [0.95335, -0.95904, -0.0, -0.0, -0.0]\n        inputs:    [-4.9646, 3.66257, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.04071, 6e-05, -0.5031, 0.0, -0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: -8.62717\n    Step 2:\n        selectors: [-0.04118, -0.00016, 0.89467, -0.15391, -0.0]\n        inputs:    [-4.9646, 3.66257, 1.0, -8.62717, 0.0]\n        G: 0.0 \u2192 computed_value: -17.25434\nOutput Selector (soft training):\n\tlogits (soft training): [0.18374, -2.97524, 2.44829]\n\tselected_node: 2\n\tintermediate_values (soft training): [3.92031, 0.32694, 3.91622]\n\tselected_value (soft training): 3.91622\nEarly stopped at step 316\nfinished:\n  - loss_train_capped: 4.2807833897117765e-15\n  - loss_train (+reg loss): 0.0014130872441455722\n  - loss_train_criterion: 0.0014130872441455722\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 316 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:22\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/z8ks77az\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233922-z8ks77az/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233922-z8ks77az\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:22\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/z8ks77az\n"
          },
          {
            "operation": "sub",
            "seed": 42,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 335,
            "duration": 29.424649000167847,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": " 1:\n        selectors: [1.0, -1.0, 1.0, 0.0, -0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.25349\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, 0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, -0.25349, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.65279, -0.42184, -2.17075]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-0.12675, -0.25349, 1.0]\n\tselected_value (hardened eval): -0.12675\nSample statistics (SOFT training state):\ninput=[1.0411, 1.69121]\noutput=-0.66495, target=-0.65012\nG (soft training): [0.98991, 0.93221, 0.08219]\n    Step 0:\n        selectors: [1.01757, -1.00523, 0.0, 0.0, 0.0]\n        inputs:    [-5.57697, -5.45023, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -0.12675\n    Step 1:\n        selectors: [0.86234, -1.25667, 0.82478, 0.0, -0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -0.25349\n    Step 2:\n        selectors: [-0.09446, 0.10698, 0.38693, 0.25218, 0.0]\n        inputs:    [-5.57697, -5.45023, -0.12675, -0.25349, 0.0]\n        G: 0.1 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [2.65279, -0.42184, -2.17075]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.62512, -1.57279, -0.40074]\n\tselected_value (soft training): -0.62512\nEarly stopped at step 335\nfinished:\n  - loss_train_capped: 5.13115025864647e-15\n  - loss_train (+reg loss): 0.0005899915704503655\n  - loss_train_criterion: 0.0005899915704503655\n  - loss_valid_inter: 1.338404484896999e-13\n  - loss_valid_extra: 7.363674150691682e-14\n\nModel (/checkpoint)  trained for 335 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:25\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/d3ku7pix\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233926-d3ku7pix/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233926-d3ku7pix\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:25\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/d3ku7pix\n"
          },
          {
            "operation": "sub",
            "seed": 99,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 300,
            "duration": 27.474493980407715,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "uted_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, -0.0, 0.0, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.00808\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 1.0, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 3.00808, 0.0]\n        G: 0.0 \u2192 computed_value: 3.00808\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.3289, 2.71322, -0.11402]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 3.00808, 3.00808]\n\tselected_value (hardened eval): 3.00808\nSample statistics (SOFT training state):\ninput=[0.04434, -1.71667]\noutput=1.77771, target=1.761\nG (soft training): [0.0119, 0.99404, 0.02227]\n    Step 0:\n        selectors: [-0.13263, 0.14676, -0.0, -0.0, 0.0]\n        inputs:    [-2.82254, -5.83063, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.04918, -1.04204, -0.04204, 0.0, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 3.00808\n    Step 2:\n        selectors: [-0.19002, -0.007, -0.16923, 0.82185, -0.0]\n        inputs:    [-2.82254, -5.83063, 1.0, 3.00808, 0.0]\n        G: 0.0 \u2192 computed_value: 3.00808\nOutput Selector (soft training):\n\tlogits (soft training): [-0.3289, 2.71322, -0.11402]\n\tselected_node: 1\n\tintermediate_values (soft training): [1.44246, 1.74384, 2.62044]\n\tselected_value (soft training): 1.74384\nEarly stopped at step 300\nfinished:\n  - loss_train_capped: 4.585487398860513e-15\n  - loss_train (+reg loss): 0.0004334343830123544\n  - loss_train_criterion: 0.0004334343830123544\n  - loss_valid_inter: 1.338404484896999e-13\n  - loss_valid_extra: 7.363674150691682e-14\n\nModel (/checkpoint)  trained for 300 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:49\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/132052i6\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233949-132052i6/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233949-132052i6\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:49\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/132052i6\n"
          },
          {
            "operation": "sub",
            "seed": 123,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 334,
            "duration": 29.450573921203613,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "puted_value: 1.0\n    Step 1:\n        selectors: [1.0, -1.0, -0.0, -0.0, -0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 8.41028\n    Step 2:\n        selectors: [-0.0, 0.0, -0.0, 1.0, 0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 8.41028, 0.0]\n        G: 0.0 \u2192 computed_value: 8.41028\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.47842, 0.27979, 2.12992]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 8.41028, 8.41028]\n\tselected_value (hardened eval): 8.41028\nSample statistics (SOFT training state):\ninput=[1.62067, 1.78536]\noutput=-0.14275, target=-0.16469\nG (soft training): [0.98318, 0.11577, 0.965]\n    Step 0:\n        selectors: [1.13978, -1.08063, -0.0, -0.0, 0.0]\n        inputs:    [4.87427, -3.53601, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.1487, 0.14661, 1.88838, 0.0, 0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 0.0, 0.0]\n        G: 0.1 \u2192 computed_value: 8.41028\n    Step 2:\n        selectors: [0.40507, -1.45034, 0.36195, -0.18034, -0.0]\n        inputs:    [4.87427, -3.53601, 1.0, 8.41028, 0.0]\n        G: 1.0 \u2192 computed_value: 8.41028\nOutput Selector (soft training):\n\tlogits (soft training): [2.09373, -0.99186, -1.08245]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.09291, 0.32042, -1.84388]\n\tselected_value (soft training): -0.09291\nEarly stopped at step 334\nfinished:\n  - loss_train_capped: 5.341872574165553e-15\n  - loss_train (+reg loss): 0.0009477154235355556\n  - loss_train_criterion: 0.0009477154235355556\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 334 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:51\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6fqvr2oy\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233951-6fqvr2oy/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233951-6fqvr2oy\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:51\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6fqvr2oy\n"
          },
          {
            "operation": "sub",
            "seed": 221,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 422,
            "duration": 33.4984347820282,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": ": 1.0\n    Step 1:\n        selectors: [1.0, -1.0, -0.0, 0.0, 0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, -0.0, -0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, -8.35478, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.93495, -2.37999, 2.44857]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, -8.35478, -8.35478]\n\tselected_value (hardened eval): -8.35478\nSample statistics (SOFT training state):\ninput=[-0.16492, 0.64124]\noutput=-0.82366, target=-0.80616\nG (soft training): [0.23355, 0.97864, 0.98882]\n    Step 0:\n        selectors: [0.05815, 0.06822, -0.0, 0.0, 0.0]\n        inputs:    [-2.67106, 5.68373, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.00154, -0.75376, -0.34887, 0.0, 0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\n    Step 2:\n        selectors: [1.22009, -1.18193, -0.067, -0.1813, -0.0]\n        inputs:    [-2.67106, 5.68373, 1.0, -8.35478, 0.0]\n        G: 1.0 \u2192 computed_value: -8.35478\nOutput Selector (soft training):\n\tlogits (soft training): [-1.93495, -2.37999, 2.44857]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.37393, -0.78345, -0.83893]\n\tselected_value (soft training): -0.83893\nEarly stopped at step 422\nfinished:\n  - loss_train_capped: 4.346448771914719e-15\n  - loss_train (+reg loss): 0.0005895174108445644\n  - loss_train_criterion: 0.0005895174108445644\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 422 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:39:55\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ksogwj81\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_233955-ksogwj81/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_233955-ksogwj81\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:39:55\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ksogwj81\n"
          },
          {
            "operation": "sub",
            "seed": 456,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 290,
            "duration": 27.390222787857056,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "6.90475\n    Step 1:\n        selectors: [-0.0, -0.0, 1.0, -0.0, -0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 6.90475\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, -0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 6.90475, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.73863, 2.70559, -1.67081]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [6.90475, 6.90475, 1.0]\n\tselected_value (hardened eval): 6.90475\nSample statistics (SOFT training state):\ninput=[0.36515, 0.55986]\noutput=-0.19925, target=-0.1947\nG (soft training): [0.22321, 0.968, 0.98699]\n    Step 0:\n        selectors: [-0.02649, 0.20779, -0.0, -0.0, -0.0]\n        inputs:    [3.26765, -3.6371, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 6.90475\n    Step 1:\n        selectors: [1.30747, -1.06975, -0.03578, 0.0, 0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 6.90475\n    Step 2:\n        selectors: [1.25142, -1.22165, -0.00185, -0.15814, 0.0]\n        inputs:    [3.26765, -3.6371, 6.90475, 6.90475, 0.0]\n        G: 1.0 \u2192 computed_value: 1.0\nOutput Selector (soft training):\n\tlogits (soft training): [-2.07113, -2.09412, 2.57976]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.41549, -0.16516, -0.20544]\n\tselected_value (soft training): -0.20544\nEarly stopped at step 290\nfinished:\n  - loss_train_capped: 3.948637207137895e-15\n  - loss_train (+reg loss): 0.0022183540277183056\n  - loss_train_criterion: 0.0022183540277183056\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 290 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:20\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/2bes8wu3\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234020-2bes8wu3/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234020-2bes8wu3\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:20\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/2bes8wu3\n"
          },
          {
            "operation": "sub",
            "seed": 345,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 428,
            "duration": 34.53119421005249,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "94\n    Step 1:\n        selectors: [0.0, -0.0, 1.0, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\n    Step 2:\n        selectors: [-0.0, -0.0, 1.0, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.98594, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.15782, -0.40129, 0.79163]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.98594, 0.98594, 0.98594]\n\tselected_value (hardened eval): 0.98594\nSample statistics (SOFT training state):\ninput=[-0.66762, -1.75979]\noutput=1.0831, target=1.09217\nG (soft training): [0.9937, 0.00906, 0.00532]\n    Step 0:\n        selectors: [0.98312, -0.99229, -0.0, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.98594\n    Step 1:\n        selectors: [0.02927, -0.00312, 0.8711, 0.0, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\n    Step 2:\n        selectors: [-0.03766, -0.00035, 0.58402, 0.46002, -0.0]\n        inputs:    [4.60163, 3.61569, 0.98594, 0.98594, 0.0]\n        G: 0.0 \u2192 computed_value: 0.98594\nOutput Selector (soft training):\n\tlogits (soft training): [-0.15782, -0.40129, 0.79163]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.08866, 1.0562, 1.0891]\n\tselected_value (soft training): 1.0891\nEarly stopped at step 428\nfinished:\n  - loss_train_capped: 3.615620274694236e-15\n  - loss_train (+reg loss): 0.0006584612419828773\n  - loss_train_criterion: 0.0006584612419828773\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 428 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:16\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u10tnkfd\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234016-u10tnkfd/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234016-u10tnkfd\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:16\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/u10tnkfd\n"
          },
          {
            "operation": "sub",
            "seed": 789,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 441,
            "duration": 34.702476024627686,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": " selectors: [-0.0, 0.0, 1.0, -0.0, 0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -8.30591\n    Step 2:\n        selectors: [1.0, -1.0, 1.0, -0.0, -0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, -8.30591, 0.0]\n        G: 1.0 \u2192 computed_value: -16.61181\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.15201, -1.1827, -0.756]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-8.30591, -8.30591, -16.61181]\n\tselected_value (hardened eval): -8.30591\nSample statistics (SOFT training state):\ninput=[-1.89626, 1.91763]\noutput=-3.78123, target=-3.81389\nG (soft training): [0.98681, 0.19733, 0.97716]\n    Step 0:\n        selectors: [1.02058, -1.00875, 0.0, -0.0, 0.0]\n        inputs:    [-4.62097, 3.68493, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -8.30591\n    Step 1:\n        selectors: [-0.10476, 0.10452, 0.99907, -0.0, 0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: -8.30591\n    Step 2:\n        selectors: [0.60862, -1.08724, 0.93549, -0.26602, -0.0]\n        inputs:    [-4.62097, 3.68493, -8.30591, -8.30591, 0.0]\n        G: 1.0 \u2192 computed_value: -16.61181\nOutput Selector (soft training):\n\tlogits (soft training): [2.15201, -1.1827, -0.756]\n\tselected_node: 0\n\tintermediate_values (soft training): [-3.76852, -0.71907, -6.01257]\n\tselected_value (soft training): -3.76852\nEarly stopped at step 441\nfinished:\n  - loss_train_capped: 4.60367404327101e-15\n  - loss_train (+reg loss): 0.00120429671369493\n  - loss_train_criterion: 0.00120429671369493\n  - loss_valid_inter: 1.0179670420409209e-13\n  - loss_valid_extra: 7.338662961825157e-14\n\nModel (/checkpoint)  trained for 441 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:28\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/au8ykvnr\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234029-au8ykvnr/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234029-au8ykvnr\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:28\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/au8ykvnr\n"
          },
          {
            "operation": "sub",
            "seed": 234,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 113.40524077415466,
            "final_inter_loss": 0.6413063407,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "omputed_value: -5.14657\n    Step 1:\n        selectors: [0.0, -1.0, 1.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -10.29313\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, -10.29313, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.72302, -1.17329, 1.1792]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [-5.14657, -10.29313, -0.6725]\n\tselected_value (hardened eval): -0.6725\nSample statistics (SOFT training state):\ninput=[-1.41493, -1.46763]\noutput=0.00409, target=0.05271\nG (soft training): [0.96114, 0.0003, 0.00459]\n    Step 0:\n        selectors: [0.0, -2e-05, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [-0.00225, 0.00379, -1.03878, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -10.29313\n    Step 2:\n        selectors: [1.02565, 0.05174, -0.00154, -0.02281, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, -10.29313, 0.0]\n        G: 0.0 \u2192 computed_value: -0.6725\nOutput Selector (soft training):\n\tlogits (soft training): [-2.55567, 0.27934, 2.05017]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.00812, 7.53012, -1.27681]\n\tselected_value (soft training): -1.27681\nfinished:\n  - loss_train_capped: 0.6895342469215393\n  - loss_train (+reg loss): 0.0016467851819470525\n  - loss_train_criterion: 0.0016467851819470525\n  - loss_valid_inter: 0.6413062810897827\n  - loss_valid_extra: 8.613598823547363\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - sub - 2025-08-22 23:40:10\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rzsfq5ak\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234010-rzsfq5ak/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234010-rzsfq5ak\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - sub - 2025-08-22 23:40:10\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rzsfq5ak\n"
          }
        ]
      },
      "mul": {
        "total_seeds": 10,
        "successful_runs": 10,
        "grokked_runs": 9,
        "success_rate": 1.0,
        "grok_rate": 0.9,
        "grok_rate_given_success": 0.9,
        "avg_grok_step": 214.77777777777777,
        "median_grok_step": 182.0,
        "best_seed_result": {
          "operation": "mul",
          "seed": 1,
          "success": true,
          "grokked": true,
          "early_stopped": true,
          "grok_step": 159,
          "duration": 20.524561882019043,
          "final_inter_loss": 0.0,
          "final_extra_loss": 0.0,
          "stdout_excerpt": "value: 0.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [0.0, 0.0, -0.0, 1.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -22.79159\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.12471, 2.06767, -1.94371]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, -22.79159, -22.79159]\n\tselected_value (hardened eval): -22.79159\nSample statistics (SOFT training state):\ninput=[0.42762, 0.16872]\noutput=0.10654, target=0.07215\nG (soft training): [0.01909, 0.93727, 0.01406]\n    Step 0:\n        selectors: [1.02841, 1.06505, 0.0, -0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.02925, -0.18103, -0.02014, -0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [0.00751, 0.20375, 1.01575, -0.26866, -0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, -22.79159, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\nOutput Selector (soft training):\n\tlogits (soft training): [0.70679, -1.2125, 1.38081]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.11279, -0.06864, 0.11646]\n\tselected_value (soft training): 0.11646\nEarly stopped at step 159\nfinished:\n  - loss_train_capped: 4.527506299555062e-15\n  - loss_train (+reg loss): 0.003092207945883274\n  - loss_train_criterion: 0.003092207945883274\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 159 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:40:47\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rpmmr9mi\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234048-rpmmr9mi/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234048-rpmmr9mi\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:40:47\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rpmmr9mi\n"
        },
        "worst_successful_result": {
          "operation": "mul",
          "seed": 42,
          "success": true,
          "grokked": false,
          "early_stopped": false,
          "grok_step": null,
          "duration": 111.71699094772339,
          "final_inter_loss": 1.1773474216,
          "final_extra_loss": Infinity,
          "stdout_excerpt": " G: 1.0 \u2192 computed_value: 9.62063\n    Step 1:\n        selectors: [-1.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -9.62063\n    Step 2:\n        selectors: [1.0, -1.0, 1.0, 1.0, 0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, -9.62063, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.15179, -3.77048, 2.93104]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [9.62063, -9.62063, -0.6725]\n\tselected_value (hardened eval): -0.6725\nSample statistics (SOFT training state):\ninput=[-0.57972, 0.83151]\noutput=-0.69888, target=-0.48204\nG (soft training): [0.96207, 0.90166, 0.99961]\n    Step 0:\n        selectors: [0.6473, 1.12744, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\n    Step 1:\n        selectors: [-1.35794, -0.84798, 0.20477, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -9.62063\n    Step 2:\n        selectors: [1.00723, -2.68528, 2.77155, 1.93641, 0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, -9.62063, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (soft training):\n\tlogits (soft training): [-0.15179, -3.77048, 2.93104]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.53276, 0.30189, -0.75655]\n\tselected_value (soft training): -0.75655\nfinished:\n  - loss_train_capped: 1.1280956268310547\n  - loss_train (+reg loss): 0.04237077385187149\n  - loss_train_criterion: 0.04237077385187149\n  - loss_valid_inter: 1.1773474216461182\n  - loss_valid_extra: 112.63907623291016\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:03\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/389dytyr\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234103-389dytyr/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234103-389dytyr\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:03\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/389dytyr\n"
        },
        "grokked_seeds": [
          1,
          99,
          7,
          123,
          221,
          234,
          345,
          456,
          789
        ],
        "failed_seeds": [],
        "non_grok_seeds": [
          42
        ],
        "all_results": [
          {
            "operation": "mul",
            "seed": 1,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 159,
            "duration": 20.524561882019043,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "value: 0.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [0.0, 0.0, -0.0, 1.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -22.79159\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.12471, 2.06767, -1.94371]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, -22.79159, -22.79159]\n\tselected_value (hardened eval): -22.79159\nSample statistics (SOFT training state):\ninput=[0.42762, 0.16872]\noutput=0.10654, target=0.07215\nG (soft training): [0.01909, 0.93727, 0.01406]\n    Step 0:\n        selectors: [1.02841, 1.06505, 0.0, -0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.02925, -0.18103, -0.02014, -0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [0.00751, 0.20375, 1.01575, -0.26866, -0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, -22.79159, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\nOutput Selector (soft training):\n\tlogits (soft training): [0.70679, -1.2125, 1.38081]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.11279, -0.06864, 0.11646]\n\tselected_value (soft training): 0.11646\nEarly stopped at step 159\nfinished:\n  - loss_train_capped: 4.527506299555062e-15\n  - loss_train (+reg loss): 0.003092207945883274\n  - loss_train_criterion: 0.003092207945883274\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 159 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:40:47\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rpmmr9mi\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234048-rpmmr9mi/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234048-rpmmr9mi\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:40:47\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/rpmmr9mi\n"
          },
          {
            "operation": "mul",
            "seed": 99,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 163,
            "duration": 20.409661054611206,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "0.0\n    Step 1:\n        selectors: [1.0, 0.0, -0.0, -0.0, 0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -3.99468\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, 0.0, 0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, -3.99468, 0.0]\n        G: 0.0 \u2192 computed_value: -14.18562\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.80659, -1.15112, 2.49592]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, -3.99468, -14.18562]\n\tselected_value (hardened eval): -14.18562\nSample statistics (SOFT training state):\ninput=[0.92211, 0.9089]\noutput=0.81928, target=0.83811\nG (soft training): [0.00652, 0.95038, 0.99417]\n    Step 0:\n        selectors: [0.99601, 1.01295, -0.0, -0.0, 0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.40487, 0.02697, 1.04134, 0.0, -0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -3.99468\n    Step 2:\n        selectors: [-1.03344, -0.19173, 1.28415, -0.11777, -0.0]\n        inputs:    [-3.99468, 3.55113, 0.0, -3.99468, 0.0]\n        G: 1.0 \u2192 computed_value: -14.18562\nOutput Selector (soft training):\n\tlogits (soft training): [3.09937, -0.73985, -1.70554]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.83296, 0.54211, -0.12396]\n\tselected_value (soft training): 0.83296\nEarly stopped at step 163\nfinished:\n  - loss_train_capped: 4.9313690677070705e-15\n  - loss_train (+reg loss): 0.00036157373688183725\n  - loss_train_criterion: 0.00036157373688183725\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 163 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:08\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6c9o3q41\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234108-6c9o3q41/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234108-6c9o3q41\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:08\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/6c9o3q41\n"
          },
          {
            "operation": "mul",
            "seed": 7,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 516,
            "duration": 38.54491996765137,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": " 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, -0.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 18.14457\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-3.75929, -0.76748, 3.59174]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, 1.0, 18.14457]\n\tselected_value (hardened eval): 18.14457\nSample statistics (SOFT training state):\ninput=[1.04053, -0.3848]\noutput=-0.40959, target=-0.40039\nG (soft training): [0.00286, 0.93055, 0.99719]\n    Step 0:\n        selectors: [1.02837, 1.00367, -0.0, -0.0, -0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-1.0577, -0.07917, 0.7205, 0.0, -0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-1.09287, -0.42293, 1.61124, 0.06445, -0.0]\n        inputs:    [-5.51524, -3.2899, 0.0, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: 18.14457\nOutput Selector (soft training):\n\tlogits (soft training): [4.73994, -0.08669, -3.36985]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.40271, -1.22035, -1.68667]\n\tselected_value (soft training): -0.40271\nEarly stopped at step 516\nfinished:\n  - loss_train_capped: 3.599882402534251e-15\n  - loss_train (+reg loss): 0.00035132712218910456\n  - loss_train_criterion: 0.00035132712218910456\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 516 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:40:51\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/os4lbfu6\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234051-os4lbfu6/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234051-os4lbfu6\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:40:51\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/os4lbfu6\n"
          },
          {
            "operation": "mul",
            "seed": 123,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 161,
            "duration": 20.54447102546692,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "puted_value: 0.0\n    Step 1:\n        selectors: [0.0, -1.0, 1.0, 0.0, 0.0]\n        inputs:    [2.88672, 4.71099, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -4.71099\n    Step 2:\n        selectors: [1.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [2.88672, 4.71099, 0.0, -4.71099, 0.0]\n        G: 0.0 \u2192 computed_value: 13.59931\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-3.19127, -1.3385, 3.37224]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, -4.71099, 13.59931]\n\tselected_value (hardened eval): 13.59931\nSample statistics (SOFT training state):\ninput=[-0.89555, -0.32758]\noutput=0.30409, target=0.29337\nG (soft training): [0.97681, 0.82121, 0.00686]\n    Step 0:\n        selectors: [-0.41278, -0.25771, -0.0, -0.0, 0.0]\n        inputs:    [2.88672, 4.71099, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.01433, -1.54208, 2.01782, 0.0, 0.0]\n        inputs:    [2.88672, 4.71099, 0.0, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: -4.71099\n    Step 2:\n        selectors: [1.02662, 1.00593, -0.01338, 0.0001, -0.0]\n        inputs:    [2.88672, 4.71099, 0.0, -4.71099, 0.0]\n        G: 0.0 \u2192 computed_value: 13.59931\nOutput Selector (soft training):\n\tlogits (soft training): [-3.19127, -1.3385, 3.37224]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.46052, 1.38681, 0.29413]\n\tselected_value (soft training): 0.29413\nEarly stopped at step 161\nfinished:\n  - loss_train_capped: 3.6281860084667536e-15\n  - loss_train (+reg loss): 0.0003029622894246131\n  - loss_train_criterion: 0.0003029622894246131\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 161 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:28\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ycdm4qkd\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234129-ycdm4qkd/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234129-ycdm4qkd\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:28\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ycdm4qkd\n"
          },
          {
            "operation": "mul",
            "seed": 221,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 159,
            "duration": 20.512435913085938,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": " selectors: [1.0, 1.0, 0.0, 0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [-0.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -4.9208\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.45805, 3.15884, -1.55981]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [-22.79159, -22.79159, -4.9208]\n\tselected_value (hardened eval): -22.79159\nSample statistics (SOFT training state):\ninput=[-1.59577, -1.54604]\noutput=2.46437, target=2.46711\nG (soft training): [0.02368, 0.00471, 0.96602]\n    Step 0:\n        selectors: [1.02244, 0.97808, -0.0, 0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 1:\n        selectors: [1.02128, 1.01136, 0.0171, 0.0, 0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -22.79159\n    Step 2:\n        selectors: [-0.22668, -0.55154, 0.2855, 0.13321, -0.0]\n        inputs:    [-4.63168, 4.9208, -22.79159, -22.79159, 0.0]\n        G: 1.0 \u2192 computed_value: -4.9208\nOutput Selector (soft training):\n\tlogits (soft training): [-1.45805, 3.15884, -1.55981]\n\tselected_node: 1\n\tintermediate_values (soft training): [2.20969, 2.46953, 2.16809]\n\tselected_value (soft training): 2.46953\nEarly stopped at step 159\nfinished:\n  - loss_train_capped: 4.328910107708871e-15\n  - loss_train (+reg loss): 0.0007698783883824944\n  - loss_train_criterion: 0.0007698783883824944\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 159 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:29\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kwbfrhx\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234129-5kwbfrhx/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234129-5kwbfrhx\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:29\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kwbfrhx\n"
          },
          {
            "operation": "mul",
            "seed": 234,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 187,
            "duration": 21.60555076599121,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "d_value: 1.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, -0.0, 0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 15.08698\n    Step 2:\n        selectors: [-0.0, -1.0, 0.0, 0.0, 0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 15.08698, 0.0]\n        G: 1.0 \u2192 computed_value: -5.92103\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.03851, 3.22703, -1.94603]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [1.0, 15.08698, -5.92103]\n\tselected_value (hardened eval): 15.08698\nSample statistics (SOFT training state):\ninput=[0.39306, -1.10819]\noutput=-0.44327, target=-0.43558\nG (soft training): [0.49798, 0.98074, 0.00364]\n    Step 0:\n        selectors: [0.04764, 0.23163, 0.0, 0.0, 0.0]\n        inputs:    [2.54803, 5.92103, 0.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.44267, -0.20585, -0.08891, 0.0, -0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 15.08698\n    Step 2:\n        selectors: [0.99961, 1.01226, -0.06017, -0.00135, -0.0]\n        inputs:    [2.54803, 5.92103, 1.0, 15.08698, 0.0]\n        G: 0.0 \u2192 computed_value: -5.92103\nOutput Selector (soft training):\n\tlogits (soft training): [-0.88063, -1.73829, 3.291]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.06224, 0.08603, -0.45261]\n\tselected_value (soft training): -0.45261\nEarly stopped at step 187\nfinished:\n  - loss_train_capped: 3.68157195403335e-15\n  - loss_train (+reg loss): 8.781938231550157e-05\n  - loss_train_criterion: 8.781938231550157e-05\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 187 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:49\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g32myxhh\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234149-g32myxhh/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234149-g32myxhh\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:49\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/g32myxhh\n"
          },
          {
            "operation": "mul",
            "seed": 345,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 182,
            "duration": 21.710609912872314,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "        selectors: [-1.0, -1.0, 0.0, -0.0, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.99502\n    Step 2:\n        selectors: [0.0, 0.0, 0.0, 1.0, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.99502, 0.0]\n        G: 0.0 \u2192 computed_value: 0.99502\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [4.07216, -2.97215, -1.00289]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-24.9113, 0.99502, 0.99502]\n\tselected_value (hardened eval): -24.9113\nSample statistics (SOFT training state):\ninput=[-1.94992, 1.26983]\noutput=-2.4601, target=-2.47607\nG (soft training): [0.00373, 0.98723, 0.23695]\n    Step 0:\n        selectors: [1.01957, 1.01815, 0.0, -0.0, 0.0]\n        inputs:    [-5.51336, 4.51835, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -24.9113\n    Step 1:\n        selectors: [-0.61524, -0.70367, 0.29789, -0.0, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.99502\n    Step 2:\n        selectors: [0.02003, 0.06741, 0.06067, 1.24399, 0.0]\n        inputs:    [-5.51336, 4.51835, -24.9113, 0.99502, 0.0]\n        G: 0.2 \u2192 computed_value: 0.99502\nOutput Selector (soft training):\n\tlogits (soft training): [4.07216, -2.97215, -1.00289]\n\tselected_node: 0\n\tintermediate_values (soft training): [-2.47516, -0.4322, -0.33454]\n\tselected_value (soft training): -2.47516\nEarly stopped at step 182\nfinished:\n  - loss_train_capped: 4.531147694195308e-15\n  - loss_train (+reg loss): 0.0002338038757443428\n  - loss_train_criterion: 0.0002338038757443428\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 182 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:50\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/4tcvernd\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234150-4tcvernd/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234150-4tcvernd\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:50\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/4tcvernd\n"
          },
          {
            "operation": "mul",
            "seed": 456,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 198,
            "duration": 22.374794006347656,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "_value: 0.0\n    Step 1:\n        selectors: [1.0, 1.0, -0.0, 0.0, 0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -21.07917\n    Step 2:\n        selectors: [1.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, -21.07917, 0.0]\n        G: 1.0 \u2192 computed_value: -4.15863\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-2.1332, 3.42358, -1.92872]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, -21.07917, -4.15863]\n\tselected_value (hardened eval): -21.07917\nSample statistics (SOFT training state):\ninput=[-0.45757, -1.81058]\noutput=0.82332, target=0.82846\nG (soft training): [0.00395, 0.9696, 0.01085]\n    Step 0:\n        selectors: [0.98303, 1.00912, 0.0, 0.0, 0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.23044, -0.05431, 0.1373, -0.0, -0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -21.07917\n    Step 2:\n        selectors: [0.07572, 0.00656, 0.99201, -0.06118, -0.0]\n        inputs:    [-4.15863, 5.06878, 0.0, -21.07917, 0.0]\n        G: 0.0 \u2192 computed_value: -4.15863\nOutput Selector (soft training):\n\tlogits (soft training): [1.945, -2.17615, 0.82329]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.82762, 0.12517, 0.8449]\n\tselected_value (soft training): 0.82762\nEarly stopped at step 198\nfinished:\n  - loss_train_capped: 4.657254806415476e-15\n  - loss_train (+reg loss): 0.00022171877208165824\n  - loss_train_criterion: 0.00022171877208165824\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 198 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:42:03\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/nrbkgr38\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234203-nrbkgr38/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234203-nrbkgr38\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:42:03\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/nrbkgr38\n"
          },
          {
            "operation": "mul",
            "seed": 789,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 208,
            "duration": 22.790745973587036,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "p 1:\n        selectors: [-1.0, 0.0, 1.0, 0.0, -0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -9.62311\n    Step 2:\n        selectors: [-0.0, -0.0, -0.0, 0.0, 0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, -9.62311, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [4.03681, -0.42867, -2.74715]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [-7.48847, -9.62311, 0.0]\n\tselected_value (hardened eval): -7.48847\nSample statistics (SOFT training state):\ninput=[1.14526, 0.66159]\noutput=0.73416, target=0.7577\nG (soft training): [0.00521, 0.85477, 0.97134]\n    Step 0:\n        selectors: [1.00689, 1.00773, -0.0, 0.0, -0.0]\n        inputs:    [2.13463, -3.50809, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -7.48847\n    Step 1:\n        selectors: [-1.69357, 0.06829, 0.50435, 0.0, -0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -9.62311\n    Step 2:\n        selectors: [-0.28491, -0.19481, -0.02066, 0.15339, 0.0]\n        inputs:    [2.13463, -3.50809, -7.48847, -9.62311, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [4.03681, -0.42867, -2.74715]\n\tselected_node: 0\n\tintermediate_values (soft training): [0.75379, -0.84649, -0.54873]\n\tselected_value (soft training): 0.75379\nEarly stopped at step 208\nfinished:\n  - loss_train_capped: 3.8381883459806705e-15\n  - loss_train (+reg loss): 0.0007111014565452933\n  - loss_train_criterion: 0.0007111014565452933\n  - loss_valid_inter: 3.8166893792299355e-15\n  - loss_valid_extra: 7.777771482420093e-13\n\nModel (/checkpoint)  trained for 208 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:42:11\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/9vujij7j\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234211-9vujij7j/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234211-9vujij7j\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:42:11\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/9vujij7j\n"
          },
          {
            "operation": "mul",
            "seed": 42,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 111.71699094772339,
            "final_inter_loss": 1.1773474216,
            "final_extra_loss": Infinity,
            "stdout_excerpt": " G: 1.0 \u2192 computed_value: 9.62063\n    Step 1:\n        selectors: [-1.0, -1.0, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -9.62063\n    Step 2:\n        selectors: [1.0, -1.0, 1.0, 1.0, 0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, -9.62063, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.15179, -3.77048, 2.93104]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [9.62063, -9.62063, -0.6725]\n\tselected_value (hardened eval): -0.6725\nSample statistics (SOFT training state):\ninput=[-0.57972, 0.83151]\noutput=-0.69888, target=-0.48204\nG (soft training): [0.96207, 0.90166, 0.99961]\n    Step 0:\n        selectors: [0.6473, 1.12744, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 9.62063\n    Step 1:\n        selectors: [-1.35794, -0.84798, 0.20477, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -9.62063\n    Step 2:\n        selectors: [1.00723, -2.68528, 2.77155, 1.93641, 0.0]\n        inputs:    [4.47407, 5.14657, 9.62063, -9.62063, 0.0]\n        G: 1.0 \u2192 computed_value: -0.6725\nOutput Selector (soft training):\n\tlogits (soft training): [-0.15179, -3.77048, 2.93104]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.53276, 0.30189, -0.75655]\n\tselected_value (soft training): -0.75655\nfinished:\n  - loss_train_capped: 1.1280956268310547\n  - loss_train (+reg loss): 0.04237077385187149\n  - loss_train_criterion: 0.04237077385187149\n  - loss_valid_inter: 1.1773474216461182\n  - loss_valid_extra: 112.63907623291016\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - mul - 2025-08-22 23:41:03\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/389dytyr\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234103-389dytyr/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234103-389dytyr\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - mul - 2025-08-22 23:41:03\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/389dytyr\n"
          }
        ]
      },
      "div": {
        "total_seeds": 10,
        "successful_runs": 9,
        "grokked_runs": 2,
        "success_rate": 0.9,
        "grok_rate": 0.2,
        "grok_rate_given_success": 0.2222222222222222,
        "avg_grok_step": 975.5,
        "median_grok_step": 975.5,
        "best_seed_result": {
          "operation": "div",
          "seed": 221,
          "success": true,
          "grokked": true,
          "early_stopped": true,
          "grok_step": 888,
          "duration": 57.510185956954956,
          "final_inter_loss": 0.0,
          "final_extra_loss": 0.0,
          "stdout_excerpt": "3, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, -1.0, 0.0, -1.0, 0.0]\n        inputs:    [4.187, 3.72613, 1.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.12368\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.15809, -2.89036, 4.01682]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.0, 1.12368]\n\tselected_value (hardened eval): 1.12368\nSample statistics (SOFT training state):\ninput=[-1.44072, 1.45898]\noutput=-0.76723, target=-0.98748\nG (soft training): [0.90698, 0.04376, 0.86417]\n    Step 0:\n        selectors: [0.01032, -1.70173, -0.0, 0.0, 0.0]\n        inputs:    [4.187, 3.72613, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.98257, -1.08267, -0.1022, 0.0, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.05451, 0.26015, 0.96024, -0.07985, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 1.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.12368\nOutput Selector (soft training):\n\tlogits (soft training): [-0.866, 2.67516, -2.26606]\n\tselected_node: 1\n\tintermediate_values (soft training): [-1.72372, -0.7392, -0.81034]\n\tselected_value (soft training): -0.7392\nEarly stopped at step 888\nfinished:\n  - loss_train_capped: 3.075113135051627e-13\n  - loss_train (+reg loss): 0.12655314803123474\n  - loss_train_criterion: 0.12655314803123474\n  - loss_valid_inter: 1.5699225773546654e-13\n  - loss_valid_extra: 3.895639202661141e-15\n\nModel (/checkpoint)  trained for 888 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:18\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/levu4yij\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234419-levu4yij/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234419-levu4yij\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:18\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/levu4yij\n"
        },
        "worst_successful_result": {
          "operation": "div",
          "seed": 99,
          "success": true,
          "grokked": false,
          "early_stopped": false,
          "grok_step": null,
          "duration": 113.58342695236206,
          "final_inter_loss": 3020671744.0,
          "final_extra_loss": Infinity,
          "stdout_excerpt": " [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, 0.0, -1.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 6123.234\n    Step 2:\n        selectors: [0.0, -1.0, 1.0, 1.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 6123.234, 0.0]\n        G: 1.0 \u2192 computed_value: 6118.08743\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.15365, -0.75076, -0.3695]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.0, 6123.234, 6118.08743]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[-1.23481, 1.20193]\noutput=-0.37765, target=-1.02736\nG (soft training): [0.66677, 0.20959, 0.71672]\n    Step 0:\n        selectors: [0.09316, 0.00458, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.7 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.31663, 2e-05, -0.58303, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 6123.234\n    Step 2:\n        selectors: [0.21604, -0.50841, 0.98374, 0.62391, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 6123.234, 0.0]\n        G: 0.7 \u2192 computed_value: 6118.08743\nOutput Selector (soft training):\n\tlogits (soft training): [0.15365, -0.75076, -0.3695]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.12379, -0.3701, -0.81114]\n\tselected_value (soft training): -0.12379\nfinished:\n  - loss_train_capped: 67135744.0\n  - loss_train (+reg loss): 26.05424690246582\n  - loss_train_criterion: 26.05424690246582\n  - loss_valid_inter: 3020672000.0\n  - loss_valid_extra: 1.4552334547042847\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:55\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/t1ownejo\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234255-t1ownejo/logs\u001b[0m\n",
          "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234255-t1ownejo\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:55\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/t1ownejo\n"
        },
        "grokked_seeds": [
          221,
          345
        ],
        "failed_seeds": [
          234
        ],
        "non_grok_seeds": [
          1,
          7,
          42,
          99,
          123,
          456,
          789
        ],
        "all_results": [
          {
            "operation": "div",
            "seed": 1,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 113.47022891044617,
            "final_inter_loss": 48.0978317261,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [0.0, -0.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 1.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [1.28518, 3.05639, -4.00801]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [-5.14657, 1.0, 0.0]\n\tselected_value (hardened eval): 1.0\nSample statistics (SOFT training state):\ninput=[1.37263, -1.28816]\noutput=-1.0598, target=-1.06557\nG (soft training): [0.00093, 0.83229, 0.00511]\n    Step 0:\n        selectors: [0.81124, 0.02538, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -5.14657\n    Step 1:\n        selectors: [-1.464, 0.38389, 0.5064, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.95783, -0.9677, -1.12308, -0.04037, -0.0]\n        inputs:    [4.47407, 5.14657, -5.14657, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [-1.36703, -4.03947, 4.45943]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.2972, -1.04083, -1.06675]\n\tselected_value (soft training): -1.06675\nfinished:\n  - loss_train_capped: 25.252925872802734\n  - loss_train (+reg loss): 14.270607948303223\n  - loss_train_criterion: 14.270607948303223\n  - loss_valid_inter: 48.097835540771484\n  - loss_valid_extra: 1.7093288898468018\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:11\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/k4dvfapw\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234211-k4dvfapw/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234211-k4dvfapw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:11\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/k4dvfapw\n"
          },
          {
            "operation": "div",
            "seed": 7,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 112.75735878944397,
            "final_inter_loss": 21.4715957642,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "0 \u2192 computed_value: 0.86933\n    Step 1:\n        selectors: [-0.0, -1.0, 1.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -4.27724\n    Step 2:\n        selectors: [-0.0, 0.0, 1.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, -4.27724, 0.0]\n        G: 1.0 \u2192 computed_value: 0.86933\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [11.77135, -3.98283, -10.16697]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.86933, -4.27724, 0.86933]\n\tselected_value (hardened eval): 0.86933\nSample statistics (SOFT training state):\ninput=[-0.29971, -0.90332]\noutput=0.34284, target=0.33179\nG (soft training): [0.90859, 0.00073, 0.00862]\n    Step 0:\n        selectors: [-0.41827, 0.28003, -0.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 0.86933\n    Step 1:\n        selectors: [0.8998, 0.05096, -0.04248, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -4.27724\n    Step 2:\n        selectors: [1.92739, -0.95579, -0.10462, -0.96444, -0.0]\n        inputs:    [4.47407, 5.14657, 0.86933, -4.27724, 0.0]\n        G: 0.0 \u2192 computed_value: 0.86933\nOutput Selector (soft training):\n\tlogits (soft training): [-7.07764, 1.75859, 6.60219]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.14512, -0.36513, 0.34842]\n\tselected_value (soft training): 0.34842\nfinished:\n  - loss_train_capped: 3.230555772781372\n  - loss_train (+reg loss): 0.006198884453624487\n  - loss_train_criterion: 0.006198884453624487\n  - loss_valid_inter: 21.47159767150879\n  - loss_valid_extra: 0.5160653591156006\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:26\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/tztwnqv3\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234226-tztwnqv3/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234226-tztwnqv3\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:26\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/tztwnqv3\n"
          },
          {
            "operation": "div",
            "seed": 42,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 114.5568151473999,
            "final_inter_loss": 25.9931411743,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 0.0, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [0.0, 0.0, -0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.16891, -0.39914, 0.56428]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 0.0, 0.0]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[-1.73653, 1.00807]\noutput=-0.07117, target=-1.72263\nG (soft training): [0.43432, 0.51297, 0.61695]\n    Step 0:\n        selectors: [0.14322, 0.00072, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.4 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.17104, 0.00192, 0.02621, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [0.13996, 0.00627, -0.03514, -0.14114, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.6 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [-0.16891, -0.39914, 0.56428]\n\tselected_node: 2\n\tintermediate_values (soft training): [0.04192, -0.05145, -0.13301]\n\tselected_value (soft training): -0.13301\nfinished:\n  - loss_train_capped: 17.813949584960938\n  - loss_train (+reg loss): 17.75869369506836\n  - loss_train_criterion: 17.75869369506836\n  - loss_valid_inter: 25.993141174316406\n  - loss_valid_extra: 0.7215508222579956\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:33\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/taaca5mu\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234233-taaca5mu/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234233-taaca5mu\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:33\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/taaca5mu\n"
          },
          {
            "operation": "div",
            "seed": 99,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 113.58342695236206,
            "final_inter_loss": 3020671744.0,
            "final_extra_loss": Infinity,
            "stdout_excerpt": " [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.0, 0.0, -1.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 6123.234\n    Step 2:\n        selectors: [0.0, -1.0, 1.0, 1.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 6123.234, 0.0]\n        G: 1.0 \u2192 computed_value: 6118.08743\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.15365, -0.75076, -0.3695]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.0, 6123.234, 6118.08743]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[-1.23481, 1.20193]\noutput=-0.37765, target=-1.02736\nG (soft training): [0.66677, 0.20959, 0.71672]\n    Step 0:\n        selectors: [0.09316, 0.00458, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.7 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [0.31663, 2e-05, -0.58303, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.2 \u2192 computed_value: 6123.234\n    Step 2:\n        selectors: [0.21604, -0.50841, 0.98374, 0.62391, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 6123.234, 0.0]\n        G: 0.7 \u2192 computed_value: 6118.08743\nOutput Selector (soft training):\n\tlogits (soft training): [0.15365, -0.75076, -0.3695]\n\tselected_node: 0\n\tintermediate_values (soft training): [-0.12379, -0.3701, -0.81114]\n\tselected_value (soft training): -0.12379\nfinished:\n  - loss_train_capped: 67135744.0\n  - loss_train (+reg loss): 26.05424690246582\n  - loss_train_criterion: 26.05424690246582\n  - loss_valid_inter: 3020672000.0\n  - loss_valid_extra: 1.4552334547042847\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:42:55\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/t1ownejo\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234255-t1ownejo/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234255-t1ownejo\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:42:55\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/t1ownejo\n"
          },
          {
            "operation": "div",
            "seed": 221,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 888,
            "duration": 57.510185956954956,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "3, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [1.0, -1.0, 0.0, -1.0, 0.0]\n        inputs:    [4.187, 3.72613, 1.0, 1.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.12368\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.15809, -2.89036, 4.01682]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [1.0, 1.0, 1.12368]\n\tselected_value (hardened eval): 1.12368\nSample statistics (SOFT training state):\ninput=[-1.44072, 1.45898]\noutput=-0.76723, target=-0.98748\nG (soft training): [0.90698, 0.04376, 0.86417]\n    Step 0:\n        selectors: [0.01032, -1.70173, -0.0, 0.0, 0.0]\n        inputs:    [4.187, 3.72613, 0.0, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.98257, -1.08267, -0.1022, 0.0, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 2:\n        selectors: [-0.05451, 0.26015, 0.96024, -0.07985, -0.0]\n        inputs:    [4.187, 3.72613, 1.0, 1.0, 0.0]\n        G: 0.9 \u2192 computed_value: 1.12368\nOutput Selector (soft training):\n\tlogits (soft training): [-0.866, 2.67516, -2.26606]\n\tselected_node: 1\n\tintermediate_values (soft training): [-1.72372, -0.7392, -0.81034]\n\tselected_value (soft training): -0.7392\nEarly stopped at step 888\nfinished:\n  - loss_train_capped: 3.075113135051627e-13\n  - loss_train (+reg loss): 0.12655314803123474\n  - loss_train_criterion: 0.12655314803123474\n  - loss_valid_inter: 1.5699225773546654e-13\n  - loss_valid_extra: 3.895639202661141e-15\n\nModel (/checkpoint)  trained for 888 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:18\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/levu4yij\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234419-levu4yij/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234419-levu4yij\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:18\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/levu4yij\n"
          },
          {
            "operation": "div",
            "seed": 234,
            "success": false,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 74.95577907562256,
            "final_inter_loss": 48.8258972168,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "2174, target=0.96873\nG (hardened eval): [1.0, 0.0, 1.0]\n    Step 0:\n        selectors: [-1.0, -0.0, -0.0, -0.0, -0.0]\n        inputs:    [4.62836, 4.77774, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -4.62836\n    Step 1:\n        selectors: [1.0, 0.0, 1.0, -0.0, 0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -21.42174\n    Step 2:\n        selectors: [-0.0, 0.0, 0.0, 0.0, 0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, -21.42174, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.85078, 4.14553, -2.58284]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [-4.62836, -21.42174, 0.0]\n\tselected_value (hardened eval): -21.42174\nSample statistics (SOFT training state):\ninput=[1.87648, -1.42256]\noutput=-1.35698, target=-1.31909\nG (soft training): [0.02574, 0.98298, 0.02045]\n    Step 0:\n        selectors: [0.56267, 0.0282, 0.0, 0.0, 0.0]\n        inputs:    [4.62836, 4.77774, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: -4.62836\n    Step 1:\n        selectors: [-0.34328, -0.44966, -0.33638, 0.0, -0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -21.42174\n    Step 2:\n        selectors: [2.94751, -0.92475, -3.32521, -0.04327, -0.0]\n        inputs:    [4.62836, 4.77774, -4.62836, -21.42174, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.07743, -4.88006, 4.35868]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.43018, -0.47835, -1.3956]\n\tselected_value (soft training): -1.3956\n> /Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py(1206)<module>()\n-> raise\n(Pdb) \n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:28\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/69hzbe7s\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234428-69hzbe7s/logs\u001b[0m\n",
            "stderr_excerpt": ", **kwargs)\n  File \"/opt/homebrew/anaconda3/envs/nalm/lib/python3.10/site-packages/torch/nn/utils/clip_grad.py\", line 104, in _get_total_norm\n    raise RuntimeError(\nRuntimeError: The total norm of order 2.0 for gradients from `parameters` is non-finite, so it cannot be clipped. To disable this error and scale the gradients by the non-finite norm anyway, set `error_if_nonfinite=False`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 1206, in <module>\n    raise\n  File \"/Users/paul_curry/ai2/nalm-benchmark/experiments/single_layer_benchmark.py\", line 1206, in <module>\n    raise\n  File \"/opt/homebrew/anaconda3/envs/nalm/lib/python3.10/bdb.py\", line 90, in trace_dispatch\n    return self.dispatch_line(frame)\n  File \"/opt/homebrew/anaconda3/envs/nalm/lib/python3.10/bdb.py\", line 115, in dispatch_line\n    if self.quitting: raise BdbQuit\nbdb.BdbQuit\n"
          },
          {
            "operation": "div",
            "seed": 345,
            "success": true,
            "grokked": true,
            "early_stopped": true,
            "grok_step": 1063,
            "duration": 67.54205107688904,
            "final_inter_loss": 0.0,
            "final_extra_loss": 0.0,
            "stdout_excerpt": "ectors: [-0.0, 1.0, -0.0, 0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: -5.73534\n    Step 2:\n        selectors: [-1.0, 1.0, -0.0, -0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, -5.73534, 0.0]\n        G: 1.0 \u2192 computed_value: -0.31501\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [2.91466, -4.8017, -0.83122]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [0.94508, -5.73534, -0.31501]\n\tselected_value (hardened eval): 0.94508\nSample statistics (SOFT training state):\ninput=[1.60332, -0.31912]\noutput=-5.00385, target=-5.02417\nG (soft training): [0.01175, 0.86361, 0.52895]\n    Step 0:\n        selectors: [1.0107, -1.0418, -0.0, 0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.94508\n    Step 1:\n        selectors: [-0.48127, 1.03278, -0.16626, 0.0, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, 0.0, 0.0]\n        G: 0.9 \u2192 computed_value: -5.73534\n    Step 2:\n        selectors: [-0.98898, 0.96823, -0.09388, -0.02884, -0.0]\n        inputs:    [-5.42033, -5.73534, 0.94508, -5.73534, 0.0]\n        G: 0.5 \u2192 computed_value: -0.31501\nOutput Selector (soft training):\n\tlogits (soft training): [2.91466, -4.8017, -0.83122]\n\tselected_node: 0\n\tintermediate_values (soft training): [-5.10275, -0.46206, -0.9013]\n\tselected_value (soft training): -5.10275\nEarly stopped at step 1063\nfinished:\n  - loss_train_capped: 5.266337902874382e-14\n  - loss_train (+reg loss): 0.12980201840400696\n  - loss_train_criterion: 0.12980201840400696\n  - loss_valid_inter: 1.5699225773546654e-13\n  - loss_valid_extra: 3.895639202661141e-15\n\nModel (/checkpoint)  trained for 1063 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:48\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/w6ydh9nw\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234449-w6ydh9nw/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234449-w6ydh9nw\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:48\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/w6ydh9nw\n"
          },
          {
            "operation": "div",
            "seed": 123,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 114.4033191204071,
            "final_inter_loss": 48.0978317261,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [0.0, 0.0, -0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [1.0, -0.0, -0.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 4.47407\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [0.37881, 0.09733, -0.37385]\n\tselected_node: 0\n\tintermediate_values (hardened eval): [1.0, 0.0, 4.47407]\n\tselected_value (hardened eval): 1.0\nSample statistics (SOFT training state):\ninput=[-0.70395, -0.97306]\noutput=0.7034, target=0.72344\nG (soft training): [0.00475, 0.00361, 0.49003]\n    Step 0:\n        selectors: [0.73604, 0.00055, 0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 1.0\n    Step 1:\n        selectors: [1.92359, -1.00867, -1.21962, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [-0.88343, 1.26615, 0.02479, 0.05287, 0.0]\n        inputs:    [4.47407, 5.14657, 1.0, 0.0, 0.0]\n        G: 0.5 \u2192 computed_value: 4.47407\nOutput Selector (soft training):\n\tlogits (soft training): [-3.46519, 3.37465, -1.79114]\n\tselected_node: 1\n\tintermediate_values (soft training): [-0.5215, 0.70887, -0.02654]\n\tselected_value (soft training): 0.70887\nfinished:\n  - loss_train_capped: 42.97352981567383\n  - loss_train (+reg loss): 26.073104858398438\n  - loss_train_criterion: 26.073104858398438\n  - loss_valid_inter: 48.097835540771484\n  - loss_valid_extra: 1.7093288898468018\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:44:05\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kra1wec\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234405-5kra1wec/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234405-5kra1wec\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:44:05\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/5kra1wec\n"
          },
          {
            "operation": "div",
            "seed": 456,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 109.55329322814941,
            "final_inter_loss": 25.9931411743,
            "final_extra_loss": Infinity,
            "stdout_excerpt": "0, 0.0, -0.0, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [1.0, 0.0, 1.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [-0.0, 0.0, 1.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-1.53748, 7.42579, -5.13841]\n\tselected_node: 1\n\tintermediate_values (hardened eval): [0.0, 0.0, 0.0]\n\tselected_value (hardened eval): 0.0\nSample statistics (SOFT training state):\ninput=[1.12516, 0.56056]\noutput=2.03133, target=2.00722\nG (soft training): [2e-05, 0.31791, 0.00465]\n    Step 0:\n        selectors: [2.9221, -0.07694, 0.0, 0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.50495, -1.2756, -0.08955, -0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.3 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [2.3504, -1.06348, -0.46812, 0.0021, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.0\nOutput Selector (soft training):\n\tlogits (soft training): [0.31267, -6.99672, 5.8817]\n\tselected_node: 2\n\tintermediate_values (soft training): [1.47573, 0.247, 2.03345]\n\tselected_value (soft training): 2.03345\nfinished:\n  - loss_train_capped: 41.38689041137695\n  - loss_train (+reg loss): 35.64674758911133\n  - loss_train_criterion: 35.64674758911133\n  - loss_valid_inter: 25.993141174316406\n  - loss_valid_extra: 0.7215508222579956\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:45:16\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ch9fjjkt\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234516-ch9fjjkt/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234516-ch9fjjkt\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:45:16\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/ch9fjjkt\nwandb: WARNING Tried to log to step 2000 that is less than the current step 2001. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n"
          },
          {
            "operation": "div",
            "seed": 789,
            "success": true,
            "grokked": false,
            "early_stopped": false,
            "grok_step": null,
            "duration": 104.72264194488525,
            "final_inter_loss": 21.4715957642,
            "final_extra_loss": Infinity,
            "stdout_excerpt": " inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.0, 0.0, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 1.0 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [1.0, -1.0, -0.0, 0.0, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.86933\nOutput Selector (hardened eval):\n\tlogits (hardened eval): [-0.28196, -2.39413, 4.0243]\n\tselected_node: 2\n\tintermediate_values (hardened eval): [0.0, 0.0, 0.86933]\n\tselected_value (hardened eval): 0.86933\nSample statistics (SOFT training state):\ninput=[1.01046, 1.60341]\noutput=0.5902, target=0.63019\nG (soft training): [0.80812, 0.84831, 0.01364]\n    Step 0:\n        selectors: [-0.23597, -0.21122, 0.0, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: 0.0\n    Step 1:\n        selectors: [-0.30398, 0.21684, 0.37334, -0.0, 0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.8 \u2192 computed_value: 0.0\n    Step 2:\n        selectors: [1.05127, -1.02366, -0.08964, 0.0144, -0.0]\n        inputs:    [4.47407, 5.14657, 0.0, 0.0, 0.0]\n        G: 0.0 \u2192 computed_value: 0.86933\nOutput Selector (soft training):\n\tlogits (soft training): [-0.28196, -2.39413, 4.0243]\n\tselected_node: 2\n\tintermediate_values (soft training): [-0.31475, -0.16243, 0.60363]\n\tselected_value (soft training): 0.60363\nfinished:\n  - loss_train_capped: 3.8393702507019043\n  - loss_train (+reg loss): 0.005665719974786043\n  - loss_train_criterion: 0.005665719974786043\n  - loss_valid_inter: 21.47159767150879\n  - loss_valid_extra: 0.5160653591156006\n\nModel (/checkpoint)  trained for 2000 epochs has been saved\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: \ud83d\ude80 View run \u001b[33mlocal - div - 2025-08-22 23:45:43\u001b[0m at: \u001b[34mhttps://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v6hd07th\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250822_234543-v6hd07th/logs\u001b[0m\n",
            "stderr_excerpt": "wandb: Currently logged in as: paul-michael-curry (paul-michael-curry-paul-curry-productions) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\nwandb: Tracking run with wandb version 0.21.1\nwandb: Run data is saved locally in /Users/paul_curry/ai2/nalm-benchmark/wandb/run-20250822_234543-v6hd07th\nwandb: Run `wandb offline` to turn off syncing.\nwandb: Syncing run local - div - 2025-08-22 23:45:43\nwandb: \u2b50\ufe0f View project at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark\nwandb: \ud83d\ude80 View run at https://wandb.ai/paul-michael-curry-paul-curry-productions/nalm-benchmark/runs/v6hd07th\n"
          }
        ]
      }
    }
  }
}